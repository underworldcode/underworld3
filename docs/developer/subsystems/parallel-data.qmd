---
title: "Parallel Data Management"
---

# Parallel Data Management

::: {.callout-note}
## Documentation In Progress
This section is under development. Parallel data management documentation is being migrated from implementation notes.
:::

## Overview

Underworld3 manages data distribution across MPI processes using PETSc's parallel data structures.

## Key Concepts

### Domain Decomposition
- Mesh partitioning across processes
- Ghost cell communication
- Halo regions for parallel consistency

### Vector Management
- Local vectors (`_lvec`) - process-local data
- Global vectors - distributed across processes
- PETSc DMPlex handles parallel assembly

### Swarm Distribution
- Particle migration between processes
- KDTree-based spatial queries
- Automatic load balancing

## Related Documentation

- [Data Access Patterns](data-access.qmd) - Synchronization mechanisms
- [Swarm System](swarm-system.qmd) - Particle migration
- [MPI Parallelism](../advanced/mpi-parallelism.qmd) - Parallel programming patterns