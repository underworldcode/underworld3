<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>underworld3.swarm API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>underworld3.swarm</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="underworld3.swarm.IndexSwarmVariable"><code class="flex name class">
<span>class <span class="ident">IndexSwarmVariable</span></span>
<span>(</span><span>name,<br>swarm,<br>indices=1,<br>proxy_degree=1,<br>proxy_continuous=True,<br>update_type=0,<br>npoints=5,<br>radius=0.5,<br>npoints_bc=2,<br>ind_bc=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IndexSwarmVariable(SwarmVariable):
    &#34;&#34;&#34;
    The IndexSwarmVariable is a class for managing material point
    behaviour. The material index variable is rendered into a
    collection of masks each representing the extent of one material
    &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        indices=1,
        proxy_degree=1,
        proxy_continuous=True,
        update_type=0,
        npoints=5,
        radius=0.5,
        npoints_bc=2,
        ind_bc=None,
    ):
        self.indices = indices
        self.nnn = npoints
        self.radius_s = radius**2
        self.update_type = update_type
        if self.update_type == 1:
            self.nnn_bc = npoints_bc
            self.ind_bc = ind_bc

        # These are the things we require of the generic swarm variable type
        super().__init__(
            name,
            swarm,
            size=1,
            vtype=None,
            dtype=int,
            _proxy=False,
        )
        &#34;&#34;&#34;
        vtype = (None,)
        dtype = (float,)
        proxy_degree = (1,)
        proxy_continuous = (True,)
        _register = (True,)
        _proxy = (True,)
        _nn_proxy = (False,)
        varsymbol = (None,)
        rebuild_on_cycle = (True,)
        &#34;&#34;&#34;
        # The indices variable defines how many &#34;level set&#34; maps we create as components in the proxy variable

        import sympy

        self._MaskArray = sympy.Matrix.zeros(1, self.indices)
        self._meshLevelSetVars = [None] * self.indices

        for i in range(indices):
            self._meshLevelSetVars[i] = uw.discretisation.MeshVariable(
                name + R&#34;^{[&#34; + str(i) + R&#34;]}&#34;,
                self.swarm.mesh,
                num_components=1,
                degree=proxy_degree,
                continuous=proxy_continuous,
            )
            self._MaskArray[0, i] = self._meshLevelSetVars[i].sym[0, 0]

        return

    # This is the sympy vector interface - it&#39;s meaningless if these are not spatial arrays
    @property
    def sym(self):
        return self._MaskArray

    @property
    def sym_1d(self):
        return self._MaskArray

    # We can  also add a __getitem__ call to access each mask

    def __getitem__(self, index):
        return self.sym[index]

    def createMask(self, funcsList):
        &#34;&#34;&#34;
        This creates a masked sympy function of swarm variables required for Underworld&#39;s solvers
        &#34;&#34;&#34;

        if not isinstance(funcsList, (tuple, list)):
            raise RuntimeError(&#34;Error input for createMask() - wrong type of input&#34;)

        if len(funcsList) != self.indices:
            raise RuntimeError(&#34;Error input for createMask() - wrong length of input&#34;)

        symo = sympy.simplify(0)
        for i in range(self.indices):
            symo += funcsList[i] * self._MaskArray[i]

        return symo

    def viewMask(self, sympy):
        &#34;&#34;&#34;
        Takes a previously masked sympy function and returns individual sympy objects corresponding to each material
        &#34;&#34;&#34;

        &#34;&#34;&#34; TODO
        output = []
        for i in range( self.indices ):
            tmp = {}
            for j in range( self.indices ):
                if i == j : pass
                tmp

        return output
        &#34;&#34;&#34;
        pass

    def visMask(self):
        return self.createMask(list(range(self.indices)))

    def view(self):
        &#34;&#34;&#34;
        Show information on IndexSwarmVariable
        &#34;&#34;&#34;
        if uw.mpi.rank == 0:
            print(f&#34;IndexSwarmVariable {self}&#34;)
            print(f&#34;Numer of indices {self.indices}&#34;)

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh (vector) variable for the index variable on the current swarm locations

        Here is how it works:

            1) for each particle, create a distance-weighted average on the node data
            2) for each index in the set, we create a mask mesh variable by mapping 1.0 wherever the
               index matches and 0.0 where it does not.

        NOTE: If no material is identified with a given nodal value, the default is to impose
        a near-neighbour hunt for a valid material and set that one

        ## ToDo: This should be revisited to match the updated master copy of _update

        update_type 0: assign the particles to the nearest mesh_levelset nodes, and calculate the value on nodes from them.
        update_type 1: calculate the material property value on mesh_levelset nodes from the nearest N particles directly.

        &#34;&#34;&#34;
        if self.update_type == 0:
            kd = uw.kdtree.KDTree(self._meshLevelSetVars[0].coords)

            with self.swarm.access():
                n_distance, n_indices = kd.query(
                    self.swarm.particle_coordinates.data, k=self.nnn
                )
                kd_swarm = uw.kdtree.KDTree(self.swarm.particle_coordinates.data)
                # n, d, b = kd_swarm.find_closest_point(self._meshLevelSetVars[0].coords)
                d, n = kd_swarm.query(
                    self._meshLevelSetVars[0].coords, k=1, sqr_dist=True
                )

            for ii in range(self.indices):
                meshVar = self._meshLevelSetVars[ii]

                with self.swarm.mesh.access(meshVar), self.swarm.access():
                    node_values = np.zeros((meshVar.data.shape[0],))
                    w = np.zeros((meshVar.data.shape[0],))

                    for i in range(self.data.shape[0]):
                        tem = np.isclose(n_distance[i, :], n_distance[i, 0])
                        dist = n_distance[i, tem]
                        indices = n_indices[i, tem]
                        tem = dist &lt; self.radius_s
                        dist = dist[tem]
                        indices = indices[tem]
                        for j, ind in enumerate(indices):
                            node_values[ind] += (
                                np.isclose(self.data[i], ii) / (1.0e-16 + dist[j])
                            )[0]
                            w[ind] += 1.0 / (1.0e-16 + dist[j])

                    node_values[np.where(w &gt; 0.0)[0]] /= w[np.where(w &gt; 0.0)[0]]
                    meshVar.data[:, 0] = node_values[...]

                    # if there is no material found,
                    # impose a near-neighbour hunt for a valid material and set that one
                    ind_w0 = np.where(w == 0.0)[0]
                    if len(ind_w0) &gt; 0:
                        ind_ = np.where(self.data[n[ind_w0]] == ii)[0]
                        if len(ind_) &gt; 0:
                            meshVar.data[ind_w0[ind_]] = 1.0
        elif self.update_type == 1:
            with self.swarm.access():
                kd = uw.kdtree.KDTree(self.swarm.particle_coordinates.data)
                n_distance, n_indices = kd.query(
                    self._meshLevelSetVars[0].coords, k=self.nnn, sqr_dist=True
                )

            for ii in range(self.indices):
                meshVar = self._meshLevelSetVars[ii]
                with self.swarm.mesh.access(meshVar), self.swarm.access():
                    node_values = np.zeros((meshVar.data.shape[0],))
                    w = np.zeros((meshVar.data.shape[0],))
                    for i in range(meshVar.data.shape[0]):
                        if i not in self.ind_bc:
                            ind = np.where(n_distance[i, :] &lt; self.radius_s)
                            a = 1.0 / (n_distance[i, ind] + 1.0e-16)
                            w[i] = np.sum(a)
                            b = np.isclose(self.data[n_indices[i, ind]], ii)
                            node_values[i] = np.sum(np.dot(a, b))
                            if ind[0].size == 0:
                                w[i] = 0
                        else:
                            ind = np.where(n_distance[i, : self.nnn_bc] &lt; self.radius_s)
                            a = 1.0 / (n_distance[i, : self.nnn_bc][ind] + 1.0e-16)
                            w[i] = np.sum(a)
                            b = np.isclose(
                                self.data[n_indices[i, : self.nnn_bc][ind]], ii
                            )
                            node_values[i] = np.sum(np.dot(a, b))
                            if ind[0].size == 0:
                                w[i] = 0

                    node_values[np.where(w &gt; 0.0)[0]] /= w[np.where(w &gt; 0.0)[0]]
                    meshVar.data[:, 0] = node_values[...]

                    # if there is no material found,
                    # impose a near-neighbour hunt for a valid material and set that one
                    ind_w0 = np.where(w == 0.0)[0]
                    if len(ind_w0) &gt; 0:
                        ind_ = np.where(self.data[n_indices[ind_w0]] == ii)[0]
                        if len(ind_) &gt; 0:
                            meshVar.data[ind_w0[ind_]] = 1.0
        return</code></pre>
</details>
<div class="desc"><p>The IndexSwarmVariable is a class for managing material point
behaviour. The material index variable is rendered into a
collection of masks each representing the extent of one material</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></li>
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.IndexSwarmVariable.sym"><code class="name">prop <span class="ident">sym</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym(self):
    return self._MaskArray</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.IndexSwarmVariable.sym_1d"><code class="name">prop <span class="ident">sym_1d</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym_1d(self):
    return self._MaskArray</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.IndexSwarmVariable.createMask"><code class="name flex">
<span>def <span class="ident">createMask</span></span>(<span>self, funcsList)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createMask(self, funcsList):
    &#34;&#34;&#34;
    This creates a masked sympy function of swarm variables required for Underworld&#39;s solvers
    &#34;&#34;&#34;

    if not isinstance(funcsList, (tuple, list)):
        raise RuntimeError(&#34;Error input for createMask() - wrong type of input&#34;)

    if len(funcsList) != self.indices:
        raise RuntimeError(&#34;Error input for createMask() - wrong length of input&#34;)

    symo = sympy.simplify(0)
    for i in range(self.indices):
        symo += funcsList[i] * self._MaskArray[i]

    return symo</code></pre>
</details>
<div class="desc"><p>This creates a masked sympy function of swarm variables required for Underworld's solvers</p></div>
</dd>
<dt id="underworld3.swarm.IndexSwarmVariable.view"><code class="name flex">
<span>def <span class="ident">view</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def view(self):
    &#34;&#34;&#34;
    Show information on IndexSwarmVariable
    &#34;&#34;&#34;
    if uw.mpi.rank == 0:
        print(f&#34;IndexSwarmVariable {self}&#34;)
        print(f&#34;Numer of indices {self.indices}&#34;)</code></pre>
</details>
<div class="desc"><p>Show information on IndexSwarmVariable</p></div>
</dd>
<dt id="underworld3.swarm.IndexSwarmVariable.viewMask"><code class="name flex">
<span>def <span class="ident">viewMask</span></span>(<span>self, sympy)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def viewMask(self, sympy):
    &#34;&#34;&#34;
    Takes a previously masked sympy function and returns individual sympy objects corresponding to each material
    &#34;&#34;&#34;

    &#34;&#34;&#34; TODO
    output = []
    for i in range( self.indices ):
        tmp = {}
        for j in range( self.indices ):
            if i == j : pass
            tmp

    return output
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>Takes a previously masked sympy function and returns individual sympy objects corresponding to each material</p></div>
</dd>
<dt id="underworld3.swarm.IndexSwarmVariable.visMask"><code class="name flex">
<span>def <span class="ident">visMask</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visMask(self):
    return self.createMask(list(range(self.indices)))</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></b></code>:
<ul class="hlist">
<li><code><a title="underworld3.swarm.SwarmVariable.save" href="#underworld3.swarm.SwarmVariable.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="underworld3.swarm.NodalPointPICSwarm"><code class="flex name class">
<span>class <span class="ident">NodalPointPICSwarm</span></span>
<span>(</span><span>trackedVariable: <function MeshVariable at 0x7f8aa634dbc0>, verbose=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NodalPointPICSwarm(PICSwarm):
    r&#34;&#34;&#34;Swarm with particles located at the coordinate points of a meshVariable

    The swarmVariable `X0` is defined so that the particles can &#34;snap back&#34; to their original locations
    after they have been moved.

    The purpose of this Swarm is to manage sample points for advection schemes based on upstream sampling
    (method of characteristics etc)&#34;&#34;&#34;

    def __init__(
        self,
        trackedVariable: uw.discretisation.MeshVariable,
        verbose=False,
    ):
        self.trackedVariable = trackedVariable
        self.swarmVariable = None

        mesh = trackedVariable.mesh

        # Set up a standard swarm
        super().__init__(mesh, verbose)

        nswarm = self

        meshVar_name = trackedVariable.clean_name
        meshVar_symbol = trackedVariable.symbol

        ks = str(self.instance_number)
        name = f&#34;{meshVar_name}_star&#34;
        symbol = rf&#34;{{ {meshVar_symbol} }}^{{ &lt;*&gt; }}&#34;

        self.swarmVariable = uw.swarm.SwarmVariable(
            name,
            nswarm,
            vtype=trackedVariable.vtype,
            _proxy=False,
            # proxy_degree=trackedVariable.degree,
            # proxy_continuous=trackedVariable.continuous,
            varsymbol=symbol,
        )

        # The launch point location
        name = f&#34;ns_X0_{ks}&#34;
        symbol = r&#34;X0^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nX0 = uw.swarm.SwarmVariable(name, nswarm, nswarm.dim, _proxy=False)

        # The launch point index
        name = f&#34;ns_I_{ks}&#34;
        symbol = r&#34;I^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nI0 = uw.swarm.SwarmVariable(name, nswarm, 1, dtype=int, _proxy=False)

        # The launch point processor rank
        name = f&#34;ns_R0_{ks}&#34;
        symbol = r&#34;R0^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nR0 = uw.swarm.SwarmVariable(name, nswarm, 1, dtype=int, _proxy=False)

        nswarm.dm.finalizeFieldRegister()
        nswarm.dm.addNPoints(
            trackedVariable.coords.shape[0] + 1
        )  # why + 1 ? That&#39;s the number of spots actually allocated

        cellid = nswarm.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = nswarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, nswarm.dim))
        coords[...] = trackedVariable.coords[...]
        cellid[:] = self.mesh.get_closest_local_cells(coords)

        # Move slightly within the chosen cell to avoid edge effects
        centroid_coords = self.mesh._centroids[cellid]

        shift = 0.001
        coords[:, :] = (1.0 - shift) * coords[:, :] + shift * centroid_coords[:, :]

        nswarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        nswarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        nswarm.dm.migrate(remove_sent_points=True)

        with nswarm.access(nX0, nI0):
            nX0.data[:, :] = coords
            nI0.data[:, 0] = range(0, coords.shape[0])

        self._nswarm = nswarm
        self._nX0 = nX0
        self._nI0 = nI0
        self._nR0 = nR0

        return

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
        evalf=False,
        step_limit=True,
    ):

        with self.access(self._X0):
            self._X0.data[...] = self._nX0.data[...]

        with self.access(self._nR0):
            self._nR0.data[...] = uw.mpi.rank

        super().advection(
            V_fn,
            delta_t,
            order,
            corrector,
            restore_points_to_domain_func,
            evalf,
            step_limit,
        )

        return</code></pre>
</details>
<div class="desc"><p>Swarm with particles located at the coordinate points of a meshVariable</p>
<p>The swarmVariable <code>X0</code> is defined so that the particles can "snap back" to their original locations
after they have been moved.</p>
<p>The purpose of this Swarm is to manage sample points for advection schemes based on upstream sampling
(method of characteristics etc)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.PICSwarm" href="#underworld3.swarm.PICSwarm">PICSwarm</a></li>
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.NodalPointPICSwarm.advection"><code class="name flex">
<span>def <span class="ident">advection</span></span>(<span>self,<br>V_fn,<br>delta_t,<br>order=2,<br>corrector=False,<br>restore_points_to_domain_func=None,<br>evalf=False,<br>step_limit=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def advection(
    self,
    V_fn,
    delta_t,
    order=2,
    corrector=False,
    restore_points_to_domain_func=None,
    evalf=False,
    step_limit=True,
):

    with self.access(self._X0):
        self._X0.data[...] = self._nX0.data[...]

    with self.access(self._nR0):
        self._nR0.data[...] = uw.mpi.rank

    super().advection(
        V_fn,
        delta_t,
        order,
        corrector,
        restore_points_to_domain_func,
        evalf,
        step_limit,
    )

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="underworld3.swarm.PICSwarm" href="#underworld3.swarm.PICSwarm">PICSwarm</a></b></code>:
<ul class="hlist">
<li><code><a title="underworld3.swarm.PICSwarm.access" href="#underworld3.swarm.PICSwarm.access">access</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.add_particles_with_coordinates" href="#underworld3.swarm.PICSwarm.add_particles_with_coordinates">add_particles_with_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.estimate_dt" href="#underworld3.swarm.PICSwarm.estimate_dt">estimate_dt</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.petsc_save_checkpoint" href="#underworld3.swarm.PICSwarm.petsc_save_checkpoint">petsc_save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.populate" href="#underworld3.swarm.PICSwarm.populate">populate</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.populate_petsc" href="#underworld3.swarm.PICSwarm.populate_petsc">populate_petsc</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.save" href="#underworld3.swarm.PICSwarm.save">save</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.write_timestep" href="#underworld3.swarm.PICSwarm.write_timestep">write_timestep</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="underworld3.swarm.NodalPointUWSwarm"><code class="flex name class">
<span>class <span class="ident">NodalPointUWSwarm</span></span>
<span>(</span><span>trackedVariable: <function MeshVariable at 0x7f8aa634dbc0>, verbose=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NodalPointUWSwarm(Swarm):
    r&#34;&#34;&#34;BASIC_Swarm with particles located at the coordinate points of a meshVariable

    The swarmVariable `X0` is defined so that the particles can &#34;snap back&#34; to their original locations
    after they have been moved.

    The purpose of this Swarm is to manage sample points for advection schemes based on upstream sampling
    (method of characteristics etc)&#34;&#34;&#34;

    def __init__(
        self,
        trackedVariable: uw.discretisation.MeshVariable,
        verbose=False,
    ):
        self.trackedVariable = trackedVariable
        self.swarmVariable = None

        mesh = trackedVariable.mesh

        # Set up a standard swarm
        super().__init__(mesh, verbose)

        nswarm = self

        meshVar_name = trackedVariable.clean_name
        meshVar_symbol = trackedVariable.symbol

        ks = str(self.instance_number)
        name = f&#34;{meshVar_name}_star&#34;
        symbol = rf&#34;{{ {meshVar_symbol} }}^{{ &lt;*&gt; }}&#34;

        self.swarmVariable = uw.swarm.SwarmVariable(
            name,
            nswarm,
            vtype=trackedVariable.vtype,
            _proxy=False,
            # proxy_degree=trackedVariable.degree,
            # proxy_continuous=trackedVariable.continuous,
            varsymbol=symbol,
        )

        # The launch point location
        name = f&#34;ns_X0_{ks}&#34;
        symbol = r&#34;X0^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nX0 = uw.swarm.SwarmVariable(name, nswarm, nswarm.dim, _proxy=False)

        # The launch point index
        name = f&#34;ns_I_{ks}&#34;
        symbol = r&#34;I^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nI0 = uw.swarm.SwarmVariable(name, nswarm, 1, dtype=int, _proxy=False)

        # The launch point processor rank
        name = f&#34;ns_R0_{ks}&#34;
        symbol = r&#34;R0^{*^{{[&#34; + ks + &#34;]}}}&#34;
        nR0 = uw.swarm.SwarmVariable(name, nswarm, 1, dtype=int, _proxy=False)

        nswarm.dm.finalizeFieldRegister()
        nswarm.dm.addNPoints(
            trackedVariable.coords.shape[0] + 1
        )  # why + 1 ? That&#39;s the number of spots actually allocated

        # cellid = nswarm.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = nswarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, nswarm.dim))
        coords[...] = trackedVariable.coords[...]

        cellid = self.mesh.get_closest_cells(
            coords,
        )

        # Move slightly within the chosen cell to avoid edge effects
        centroid_coords = self.mesh._centroids[cellid]

        shift = 0.001
        coords[:, :] = (1.0 - shift) * coords[:, :] + shift * centroid_coords[:, :]

        nswarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        # nswarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        nswarm.dm.migrate(remove_sent_points=True)

        with nswarm.access(nX0, nI0):
            nX0.data[:, :] = coords
            nI0.data[:, 0] = range(0, coords.shape[0])

        self._nswarm = nswarm
        self._nX0 = nX0
        self._nI0 = nI0
        self._nR0 = nR0

        return

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
        evalf=False,
        step_limit=True,
    ):

        with self.access(self._X0):
            self._X0.data[...] = self._nX0.data[...]

        with self.access(self._nR0):
            self._nR0.data[...] = uw.mpi.rank

        super().advection(
            V_fn,
            delta_t,
            order,
            corrector,
            restore_points_to_domain_func,
            evalf,
            step_limit,
        )

        return</code></pre>
</details>
<div class="desc"><p>BASIC_Swarm with particles located at the coordinate points of a meshVariable</p>
<p>The swarmVariable <code>X0</code> is defined so that the particles can "snap back" to their original locations
after they have been moved.</p>
<p>The purpose of this Swarm is to manage sample points for advection schemes based on upstream sampling
(method of characteristics etc)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></li>
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.NodalPointUWSwarm.advection"><code class="name flex">
<span>def <span class="ident">advection</span></span>(<span>self,<br>V_fn,<br>delta_t,<br>order=2,<br>corrector=False,<br>restore_points_to_domain_func=None,<br>evalf=False,<br>step_limit=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def advection(
    self,
    V_fn,
    delta_t,
    order=2,
    corrector=False,
    restore_points_to_domain_func=None,
    evalf=False,
    step_limit=True,
):

    with self.access(self._X0):
        self._X0.data[...] = self._nX0.data[...]

    with self.access(self._nR0):
        self._nR0.data[...] = uw.mpi.rank

    super().advection(
        V_fn,
        delta_t,
        order,
        corrector,
        restore_points_to_domain_func,
        evalf,
        step_limit,
    )

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></b></code>:
<ul class="hlist">
<li><code><a title="underworld3.swarm.Swarm.access" href="#underworld3.swarm.Swarm.access">access</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_particles_with_coordinates" href="#underworld3.swarm.Swarm.add_particles_with_coordinates">add_particles_with_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_particles_with_global_coordinates" href="#underworld3.swarm.Swarm.add_particles_with_global_coordinates">add_particles_with_global_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.estimate_dt" href="#underworld3.swarm.Swarm.estimate_dt">estimate_dt</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.migrate" href="#underworld3.swarm.Swarm.migrate">migrate</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.petsc_save_checkpoint" href="#underworld3.swarm.Swarm.petsc_save_checkpoint">petsc_save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.populate" href="#underworld3.swarm.Swarm.populate">populate</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.save" href="#underworld3.swarm.Swarm.save">save</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.write_timestep" href="#underworld3.swarm.Swarm.write_timestep">write_timestep</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="underworld3.swarm.PICSwarm"><code class="flex name class">
<span>class <span class="ident">PICSwarm</span></span>
<span>(</span><span>mesh, recycle_rate=0, verbose=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PICSwarm(Stateful, uw_object):
    &#34;&#34;&#34;
    Particle swarm implementation with automatic mesh-particle interactions.

    The `Swarm` class is Underworld&#39;s primary particle management system, built on PETSc&#39;s
    DMSWARM_PIC type. It provides automatic particle migration, mesh-particle connectivity,
    and streamlined particle operations for Lagrangian particle tracking and data storage.

    Differences from UW Swarm:
    - **Mesh Integration**: Built-in particle-in-cell (PIC) connectivity with automatic cell tracking
    - **Migration**: Uses the standard PETSc strategy for migration which depends on the DM type. This requires
      calculation of cell-relationships each time the coordinates are updated and particles that are not found will
      be deleted.


    Parameters
    ----------
    mesh : uw.discretisation.Mesh
        The mesh object that defines the computational domain. Particles will be
        automatically associated with mesh cells for efficient spatial operations.
    recycle_rate : int, optional
        Rate at which particles are recycled for streak management. If &gt; 1, enables
        streak particle functionality where particles are duplicated and tracked
        across multiple cycles. Default is 0 (no recycling).
    verbose : bool, optional
        Enable verbose output for debugging and monitoring particle operations.
        Default is False.

    Attributes
    ----------
    mesh : uw.discretisation.Mesh
        Reference to the associated mesh object.
    dim : int
        Spatial dimension of the mesh (2D or 3D).
    cdim : int
        Coordinate dimension of the mesh.
    data : numpy.ndarray
        Direct access to particle coordinate data.
    particle_coordinates : SwarmVariable
        SwarmVariable containing particle coordinate information (auto-created).
    particle_cellid : SwarmVariable
        SwarmVariable containing particle cell ID information (auto-created).
    recycle_rate : int
        Current recycle rate for streak management.
    cycle : int
        Current cycle number for streak particles.

    Methods
    -------
    populate_petsc(fill_param=1)
        Populate swarm using PETSc&#39;s built-in particle generation.
    populate(fill_param=1, layout=SwarmPICLayout.GAUSS)
        Populate the swarm with particles using specified layout.
    add_particles_with_coordinates(coords)
        Add new particles at specified coordinate locations.
    add_variable(name, size, dtype=float)
        Add a new variable to track additional particle properties.
    save(filename, meshUnits=1.0, swarmUnits=1.0, units=&#34;dimensionless&#34;)
        Save swarm data to file.
    read_timestep(filename, step_name, outputPath=&#34;./output/&#34;)
        Read swarm data from a specific timestep file.
    advection(V_fn, delta_t, evalf=False, corrector=True, restore_points_func=None)
        Advect particles using a velocity field with automatic migration.
    estimate_dt(V_fn, dt_min=1.0e-15, dt_max=1.0)
        Estimate appropriate timestep for particle advection.

    Examples
    --------
    Create a standard swarm with automatic features:

    &gt;&gt;&gt; import underworld3 as uw
    &gt;&gt;&gt; mesh = uw.meshing.UnstructuredSimplexBox(minCoords=(0,0), maxCoords=(1,1))
    &gt;&gt;&gt; swarm = uw.swarm.Swarm(mesh=mesh)
    &gt;&gt;&gt; swarm.populate(fill_param=2, layout=uw.swarm.SwarmPICLayout.GAUSS)

    Access automatic coordinate and cell ID fields:

    &gt;&gt;&gt; coords = swarm.particle_coordinates.data
    &gt;&gt;&gt; cell_ids = swarm.particle_cellid.data

    Create a streak swarm with recycling:

    &gt;&gt;&gt; streak_swarm = uw.swarm.Swarm(mesh=mesh, recycle_rate=5)
    &gt;&gt;&gt; streak_swarm.populate(fill_param=1)

    Add custom particle data and perform advection:

    &gt;&gt;&gt; temperature = swarm.add_variable(&#34;temperature&#34;, 1)
    &gt;&gt;&gt; velocity_field = mesh.add_variable(&#34;velocity&#34;, mesh.dim)
    &gt;&gt;&gt; # ... set up velocity field ...
    &gt;&gt;&gt; swarm.advection(velocity_field.sym, delta_t=0.01)  # Automatic migration

    Notes
    -----
    - Particle migration occurs automatically during advection operations
    - Coordinate and cell ID fields are created and managed automatically at the
      PETSc level


    &#34;&#34;&#34;

    instances = 0

    @timing.routine_timer_decorator
    def __init__(self, mesh, recycle_rate=0, verbose=False):
        Swarm.instances += 1

        self.celldm = mesh.dm.clone()

        self.verbose = verbose
        self._mesh = mesh
        self.dim = mesh.dim
        self.cdim = mesh.cdim
        self.dm = PETSc.DMSwarm().create()
        self.dm.setDimension(self.dim)
        self.dm.setType(SwarmType.DMSWARM_PIC.value)
        self.dm.setCellDM(self.celldm)
        self._data = None

        # Is the swarm a streak-swarm ?
        self.recycle_rate = recycle_rate
        self.cycle = 0

        # dictionary for variables

        # import weakref (not helpful as garbage collection does not remove the fields from the DM)
        # self._vars = weakref.WeakValueDictionary()
        self._vars = {}

        # add variable to handle particle coords - predefined by DMSwarm, expose to UW
        self._coord_var = SwarmVariable(
            &#34;DMSwarmPIC_coor&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to handle particle cell id - predefined by DMSwarm, expose to UW
        self._cellid_var = SwarmVariable(
            &#34;DMSwarm_cellid&#34;,
            self,
            1,
            dtype=int,
            _register=False,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to hold swarm coordinates during position updates
        self._X0 = uw.swarm.SwarmVariable(
            &#34;DMSwarm_X0&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=True,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # This is for swarm streak management:
        # add variable to hold swarm origins

        if self.recycle_rate &gt; 1:
            # self._Xorig = uw.swarm.SwarmVariable(
            #     &#34;DMSwarm_Xorig&#34;,
            #     self,
            #     self.cdim,
            #     dtype=float,
            #     _register=True,
            #     _proxy=False,
            #     rebuild_on_cycle=False,
            # )

            self._remeshed = uw.swarm.SwarmVariable(
                &#34;DMSwarm_remeshed&#34;,
                self,
                1,
                dtype=int,
                _register=True,
                _proxy=False,
                rebuild_on_cycle=False,
            )

        self._X0_uninitialised = True
        self._index = None
        self._nnmapdict = {}

        super().__init__()

    @property
    def mesh(self):
        return self._mesh

    @property
    def data(self):
        return self.particle_coordinates.data

    @property
    def particle_coordinates(self):
        return self._coord_var

    @property
    def particle_cellid(self):
        return self._cellid_var

    @timing.routine_timer_decorator
    def populate_petsc(
        self,
        fill_param: Optional[int] = 3,
        layout: Optional[SwarmPICLayout] = None,
    ):
        &#34;&#34;&#34;
        Populate the swarm with particles throughout the domain.

        When using SwarmPICLayout.REGULAR,     `fill_param` defines the number of points in each spatial direction.
        When using SwarmPICLayout.GAUSS,       `fill_param` defines the number of quadrature points in each spatial direction.
        When using SwarmPICLayout.SUBDIVISION, `fill_param` defines the number times the reference cell is sub-divided.

        Parameters
        ----------
        fill_param:
            Parameter determining the particle count per cell for the given layout.
        layout:
            Type of layout to use. Defaults to `SwarmPICLayout.REGULAR` for mesh objects with simplex
            type cells, and `SwarmPICLayout.GAUSS` otherwise.

        &#34;&#34;&#34;

        self.fill_param = fill_param

        &#34;&#34;&#34;
        Currently (2021.11.15) supported by PETSc release 3.16.x

        When using a DMPLEX the following case are supported:
              (i) DMSWARMPIC_LAYOUT_REGULAR: 2D (triangle),
             (ii) DMSWARMPIC_LAYOUT_GAUSS: 2D and 3D provided the cell is a tri/tet or a quad/hex,
            (iii) DMSWARMPIC_LAYOUT_SUBDIVISION: 2D and 3D for quad/hex and 2D tri.

        So this means, simplex mesh in 3D only supports GAUSS - This is based
        on the tensor product locations so it is not uniform in the cells.
        &#34;&#34;&#34;

        if layout == None:
            layout = SwarmPICLayout.GAUSS

        if not isinstance(layout, SwarmPICLayout):
            raise ValueError(&#34;&#39;layout&#39; must be an instance of &#39;SwarmPICLayout&#39;&#34;)

        self.layout = layout
        self.dm.finalizeFieldRegister()

        ## Commenting this out for now.
        ## Code seems to operate fine without it, and the
        ## existing values are wrong. It should be something like
        ## `(elend-elstart)*fill_param^dim` for quads, and around
        ## half that for simplices, depending on layout.
        # elstart,elend = self.mesh.dm.getHeightStratum(0)
        # self.dm.setLocalSizes((elend-elstart) * fill_param, 0)

        self.dm.insertPointUsingCellDM(self.layout.value, fill_param)
        return

    #

    @timing.routine_timer_decorator
    def populate(
        self,
        fill_param: Optional[int] = 1,
    ):
        &#34;&#34;&#34;
        Populate the swarm with particles throughout the domain.

        Parameters
        ----------
        fill_param:
            Parameter determining the particle count per cell (per dimension)
            for the given layout, using the mesh degree.

        cell_search:
            Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)

        &#34;&#34;&#34;

        self.fill_param = fill_param

        newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
        newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

        if np.any(newp_cells0 &gt; self.mesh._centroids.shape[0]):
            raise RuntimeError(&#34;Some new coordinates can&#39;t find a owning cell - Error&#34;)

        # valid = newp_cells0 != -1
        # newp_coords = newp_coords0[valid]
        # newp_cells = newp_cells0[valid]
        newp_coords = newp_coords0
        newp_cells = newp_cells0

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(newp_coords.shape[0] + 1)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = newp_coords[...]
        cellid[:] = newp_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        ## Now make a series of copies to allow the swarm cycling to
        ## work correctly (if required)

        # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        # lost = np.where(cellid == -1)
        # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        if self.recycle_rate &gt; 1:
            with self.access():
                # This is a mesh-local quantity, so let&#39;s just
                # store it on the mesh in an ad_hoc fashion for now

                self.mesh.particle_X_orig = self.particle_coordinates.data.copy()
                self.mesh.particle_CellID_orig = self._cellid_var.data.copy()

            with self.access():
                swarm_orig_size = self.particle_coordinates.data.shape[0]
                all_local_coords = np.vstack(
                    (self.particle_coordinates.data,) * (self.recycle_rate)
                )
                all_local_cells = np.vstack(
                    (self._cellid_var.data,) * (self.recycle_rate)
                )

                swarm_new_size = all_local_coords.data.shape[0]

            self.dm.addNPoints(swarm_new_size - swarm_orig_size)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

            coords[...] = (
                all_local_coords[...]
                + (0.33 / (1 + fill_param))
                * (np.random.random(size=all_local_coords.shape) - 0.5)
                * 0.00001
                * self.mesh._search_lengths[all_local_cells]  # typical cell size
            )
            cellid[:] = all_local_cells[:, 0]

            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

            ## Now set the cycle values

            with self.access(self._remeshed):
                for i in range(0, self.recycle_rate):
                    offset = swarm_orig_size * i
                    self._remeshed.data[offset::, 0] = i

        return

    @timing.routine_timer_decorator
    def add_particles_with_coordinates(self, coordinatesArray) -&gt; int:
        &#34;&#34;&#34;
        Add particles to the swarm using particle coordinates provided
        using a numpy array.

        Note that particles with coordinates NOT local to the current processor will
        be rejected / ignored.

        Either include an array with all coordinates to all processors
        or an array with the local coordinates.

        Parameters
        ----------
        coordinatesArray : numpy.ndarray
            The numpy array containing the coordinate of the new particles. Array is
            expected to take shape n*dim, where n is the number of new particles, and
            dim is the dimensionality of the swarm&#39;s supporting mesh.

        Returns
        --------
        npoints: int
            The number of points added to the local section of the swarm.
        &#34;&#34;&#34;

        if not isinstance(coordinatesArray, np.ndarray):
            raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
        if not len(coordinatesArray.shape) == 2:
            raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
        if not coordinatesArray.shape[1] == self.mesh.dim:
            #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
            raise ValueError(
                &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                              number of particles to add, and &#39;dim&#39; is the dimensionality of
                              the supporting mesh ({}).&#34;&#34;&#34;.format(
                    self.mesh.dim
                )
            )

        cells = self.mesh.get_closest_local_cells(coordinatesArray)

        valid_coordinates = coordinatesArray[cells != -1]
        valid_cells = cells[cells != -1]

        npoints = len(valid_coordinates)
        swarm_size = self.dm.getLocalSize()

        # -1 means no particles have been added yet
        if swarm_size == -1:
            swarm_size = 0
            npoints = npoints + 1

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(npoints=npoints)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[swarm_size::, :] = valid_coordinates[:, :]
        cellid[swarm_size::] = valid_cells[:]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        # Here we update the swarm cycle values as required

        if self.recycle_rate &gt; 1:
            with self.access(self._remeshed):
                # self._Xorig.data[...] = coordinatesArray
                self._remeshed.data[...] = 0

        self.dm.migrate(remove_sent_points=True)

        return npoints

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm coordinates to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm checkpoint file to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

        if h5py.h5.get_config().mpi == True and not force_sequential:
            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores. This is probably why the parallel
            # h5py write hangs

            with self.access():
                data_copy = self.data[:].copy()

            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy[:],
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])

            del data_copy

        else:
            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores

            with self.access():
                data_copy = self.data[:].copy()

            if comm.rank == 0:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                        )

            comm.barrier()
            for i in range(1, comm.size):
                if comm.rank == i:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;coordinates&#34;].resize(
                            (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                            axis=0,
                        )
                        # passive swarm, zero local particles is not unusual
                        if data_copy.shape[0] &gt; 0:
                            h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
                comm.barrier()
            comm.barrier()

            del data_copy

        return

    @timing.routine_timer_decorator
    def read_timestep(
        self,
        base_filename: str,
        swarm_id: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):
        output_base_name = os.path.join(outputPath, base_filename)
        swarm_file = output_base_name + f&#34;.{swarm_id}.{index:05}.h5&#34;

        ### open up file with coords on all procs
        with h5py.File(f&#34;{swarm_file}&#34;, &#34;r&#34;) as h5f:
            coordinates = h5f[&#34;coordinates&#34;][:]

        #### utilises the UW function for adding a swarm by an array
        self.add_particles_with_coordinates(coordinates)

        return

    @timing.routine_timer_decorator
    def add_variable(
        self,
        name,
        size=1,
        dtype=float,
        proxy_degree=2,
        _nn_proxy=False,
    ):
        return SwarmVariable(
            name,
            self,
            size,
            dtype=dtype,
            proxy_degree=proxy_degree,
            _nn_proxy=_nn_proxy,
        )

    @timing.routine_timer_decorator
    def petsc_save_checkpoint(
        self,
        swarmName: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):
        &#34;&#34;&#34;

        Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        &#34;&#34;&#34;

        x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:05d}.xmf&#34;
        self.dm.viewXDMF(x_swarm_fname)

    @timing.routine_timer_decorator
    def write_timestep(
        self,
        filename: str,
        swarmname: str,
        index: int,
        swarmVars: Optional[list] = None,
        outputPath: Optional[str] = &#34;&#34;,
        time: Optional[int] = None,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential: Optional[bool] = False,
    ):
        &#34;&#34;&#34;

        Save data to h5 and a corresponding xdmf for visualisation using h5py.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        swarmVars :
            List of swarm objects to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        time :
            Attach the time to the generated xdmf.
        compression :
            Whether to compress the h5 files [bool].
        compressionType :
            The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
        &#34;&#34;&#34;

        # This will eliminate the issue of whether or not to put path separators in the
        # outputPath. Also does the right thing if outputPath is &#34;&#34;

        output_base_name = os.path.join(outputPath, filename) + &#34;.&#34; + swarmname

        # check the directory where we will write checkpoint
        dir_path = os.path.dirname(output_base_name)  # get directory

        # check if path exists
        if os.path.exists(os.path.abspath(dir_path)):  # easier to debug abs
            pass
        else:
            raise RuntimeError(f&#34;{os.path.abspath(dir_path)} does not exist&#34;)

        # check if we have write access
        if os.access(os.path.abspath(dir_path), os.W_OK):
            pass
        else:
            raise RuntimeError(f&#34;No write access to {os.path.abspath(dir_path)}&#34;)

        # could also try to coerce this to be a list and raise if it fails (tuple, singleton ... )
        # also ... why the typechecking if this can still happen

        if swarmVars is not None and not isinstance(swarmVars, list):
            raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

        else:
            ### save the swarm particle location
            self.save(
                filename=f&#34;{output_base_name}.{index:05d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

        #### Generate a h5 file for each field
        if swarmVars != None:
            for field in swarmVars:
                field.save(
                    filename=f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;,
                    compression=compression,
                    compressionType=compressionType,
                    force_sequential=force_sequential,
                )

        if uw.mpi.rank == 0:
            ### only need to combine the h5 files to a single xdmf on one proc
            with open(f&#34;{output_base_name}.{index:05d}.xdmf&#34;, &#34;w&#34;) as xdmf:
                # Write the XDMF header
                xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
                xdmf.write(
                    &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
                )
                xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
                xdmf.write(
                    f&#39;&lt;Grid Name=&#34;{output_base_name}.{index:05d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
                )

                if time != None:
                    xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

                # Write the grid element for the HDF5 dataset
                with h5py.File(f&#34;{output_base_name}.{index:05}.h5&#34;, &#34;r&#34;) as h5f:
                    xdmf.write(
                        f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                    )
                    if h5f[&#34;coordinates&#34;].shape[1] == 2:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                    elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                    xdmf.write(
                        f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                    )
                    xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

                # Write the attribute element for the field
                if swarmVars != None:
                    for field in swarmVars:
                        with h5py.File(
                            f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;, &#34;r&#34;
                        ) as h5f:
                            if h5f[&#34;data&#34;].dtype == np.int32:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 1:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            else:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )

                            xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
                else:
                    pass

                # Write the XDMF footer
                xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)

    @property
    def vars(self):
        return self._vars

    def access(self, *writeable_vars: SwarmVariable):
        &#34;&#34;&#34;
        This context manager makes the underlying swarm variables data available to
        the user. The data should be accessed via the variables `data` handle.

        As default, all data is read-only. To enable writeable data, the user should
        specify which variable they wish to modify.

        At the conclusion of the users context managed block, numerous further operations
        will be automatically executed. This includes swarm parallel migration routines
        where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
        variable proxy mesh variables will also be updated for modifed swarm variables.

        Parameters
        ----------
        writeable_vars
            The variables for which data write access is required.

        Example
        -------

        &gt;&gt;&gt; import underworld3 as uw
        &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
        &gt;&gt;&gt; with someMesh.deform_mesh():
        ...     someMesh.data[0] = [0.1,0.1]
        &gt;&gt;&gt; someMesh.data[0]
        array([ 0.1,  0.1])
        &#34;&#34;&#34;
        import time

        uw.timing._incrementDepth()
        stime = time.time()

        deaccess_list = []
        for var in self._vars.values():
            # if already accessed within higher level context manager, continue.
            if var._is_accessed == True:
                continue
            # set flag so variable status can be known elsewhere
            var._is_accessed = True
            # add to de-access list to rewind this later
            deaccess_list.append(var)
            # grab numpy object, setting read only if necessary
            var._data = self.dm.getField(var.clean_name).reshape(
                (-1, var.num_components)
            )
            assert var._data is not None
            if var not in writeable_vars:
                var._old_data_flag = var._data.flags.writeable
                var._data.flags.writeable = False
            else:
                # increment variable state
                var._increment()

            # make view for each var component
            if var._proxy:
                for i in range(0, var.shape[0]):
                    for j in range(0, var.shape[1]):
                        var._data_container[i, j] = var._data_container[i, j]._replace(
                            data=var.data[:, var._data_layout(i, j)],
                        )

        # if particles moving, update swarm state
        if self.particle_coordinates in writeable_vars:
            self._increment()

        # Create a class which specifies the required context
        # manager hooks (`__enter__`, `__exit__`).
        class exit_manager:
            def __init__(self, swarm):
                self.em_swarm = swarm

            def __enter__(self):

                pass

            def __exit__(self, *args):

                for var in self.em_swarm.vars.values():
                    # only de-access variables we have set access for.
                    if var not in deaccess_list:
                        continue
                    # set this back, although possibly not required.
                    if var not in writeable_vars:
                        var._data.flags.writeable = var._old_data_flag
                    var._data = None
                    self.em_swarm.dm.restoreField(var.clean_name)
                    var._is_accessed = False
                # do particle migration if coords changes

                if self.em_swarm.particle_coordinates in writeable_vars:
                    # let&#39;s use the mesh index to update the particles owning cells.
                    # note that the `petsc4py` interface is more convenient here as the
                    # `SwarmVariable.data` interface is controlled by the context manager
                    # that we are currently within, and it is therefore too easy to
                    # get things wrong that way.

                    cellid = self.em_swarm.dm.getField(&#34;DMSwarm_cellid&#34;)
                    coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                        (-1, self.em_swarm.dim)
                    )

                    cellid[:] = self.em_swarm.mesh.get_closest_cells(coords).reshape(-1)

                    # num_lost = np.where(cellid == -1)[0].shape[0]
                    # print(
                    #     f&#34;{uw.mpi.rank} - EM 1: illegal_cells - {num_lost}&#34;, flush=True
                    # )

                    # if num_lost != 0:
                    #     print(&#34;LOST: &#34;, coords[np.where(cellid == -1)])

                    self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                    self.em_swarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)
                    # now migrate.

                    self.em_swarm.dm.migrate(remove_sent_points=True)

                    # void these things too
                    self.em_swarm._index = None
                    self.em_swarm._nnmapdict = {}

                # do var updates
                for var in self.em_swarm.vars.values():
                    # if swarm migrated, update all.
                    # if var updated, update var.
                    if (self.em_swarm.particle_coordinates in writeable_vars) or (
                        var in writeable_vars
                    ):
                        var._update()

                    if var._proxy:
                        for i in range(0, var.shape[0]):
                            for j in range(0, var.shape[1]):
                                # var._data_ij[i, j] = None
                                var._data_container[i, j] = var._data_container[
                                    i, j
                                ]._replace(
                                    data=f&#34;SwarmVariable[...].data is only available within mesh.access() context&#34;,
                                )

                uw.timing._decrementDepth()
                uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

        return exit_manager(self)

    ## Better to have one master copy - this one is cut&#39;n&#39;pasted from
    ## the MeshVariable class

    def _data_layout(self, i, j=None):
        # mapping

        if self.vtype == uw.VarType.SCALAR:
            return 0
        if self.vtype == uw.VarType.VECTOR:
            if j is None:
                return i
            elif i == 0:
                return j
            else:
                raise IndexError(
                    f&#34;Vectors have shape {self.mesh.dim} or {(1, self.mesh.dim)} &#34;
                )
        if self.vtype == uw.VarType.TENSOR:
            if self.mesh.dim == 2:
                return ((0, 1), (2, 3))[i][j]
            else:
                return ((0, 1, 2), (3, 4, 5), (6, 7, 8))[i][j]

        if self.vtype == uw.VarType.SYM_TENSOR:
            if self.mesh.dim == 2:
                return ((0, 2), (2, 1))[i][j]
            else:
                return ((0, 3, 4), (3, 1, 5), (4, 5, 2))[i][j]

        if self.vtype == uw.VarType.MATRIX:
            return i + j * self.shape[0]

    @timing.routine_timer_decorator
    def _get_map(self, var):
        # generate tree if not avaiable
        if not self._index:
            with self.access():
                self._index = uw.kdtree.KDTree(self.data)

        # get or generate map
        meshvar_coords = var._meshVar.coords
        # we can&#39;t use numpy arrays directly as keys in python dicts, so
        # we&#39;ll use `xxhash` to generate a hash of array.
        # this shouldn&#39;t be an issue performance wise but we should test to be
        # sufficiently confident of this.
        import xxhash

        h = xxhash.xxh64()
        h.update(meshvar_coords)
        digest = h.intdigest()
        if digest not in self._nnmapdict:
            # self._nnmapdict[digest] = self._index.find_closest_point(meshvar_coords)[0]
            self._nnmapdict[digest] = self._index.query(meshvar_coords, k=1)[0]
        return self._nnmapdict[digest]

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
        evalf=False,
        step_limit=True,
    ):

        dt_limit = self.estimate_dt(V_fn)

        if step_limit and dt_limit is not None:
            substeps = int(max(1, round(abs(delta_t) / dt_limit)))
        else:
            substeps = 1

        if uw.mpi.rank == 0 and self.verbose:
            print(f&#34;Substepping {substeps} / {abs(delta_t) / dt_limit}, {delta_t} &#34;)

        # X0 holds the particle location at the start of advection
        # This is needed because the particles may be migrated off-proc
        # during timestepping.

        X0 = self._X0

        V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

        # Use current velocity to estimate where the particles would have
        # landed in an implicit step. WE CANT DO THIS WITH SUB-STEPPING unless
        # We have a lot more information about the previous launch point / timestep
        # Also: how does this interact with the particle restoration function ?

        # if corrector == True and not self._X0_uninitialised:
        #     with self.access(self.particle_coordinates):
        #         v_at_Vpts = np.zeros_like(self.data)

        #         if evalf:
        #             for d in range(self.dim):
        #                 v_at_Vpts[:, d] = uw.function.evalf(
        #                     V_fn_matrix[d], self.data
        #                 ).reshape(-1)
        #         else:
        #             for d in range(self.dim):
        #                 v_at_Vpts[:, d] = uw.function.evaluate(
        #                     V_fn_matrix[d], self.data
        #                 ).reshape(-1)

        #         corrected_position = X0.data.copy() + delta_t * v_at_Vpts
        #         if restore_points_to_domain_func is not None:
        #             corrected_position = restore_points_to_domain_func(
        #                 corrected_position
        #             )

        #         updated_current_coords = 0.5 * (corrected_position + self.data.copy())

        #         # validate_coords to ensure they live within the domain (or there will be trouble)

        #         if restore_points_to_domain_func is not None:
        #             updated_current_coords = restore_points_to_domain_func(
        #                 updated_current_coords
        #             )

        #         self.data[...] = updated_current_coords[...]

        #         del updated_current_coords
        #         del v_at_Vpts

        print(f&#34;{ uw.mpi.rank}: Peace&#34;, flush=True)

        # Wrap this whole thing in sub-stepping loop
        for step in range(0, substeps):

            with self.access(X0):
                X0.data[...] = self.particle_coordinates.data[...]

            # Mid point algorithm (2nd order)

            if order == 2:
                with self.access(self.particle_coordinates):
                    v_at_Vpts = np.zeros_like(self.particle_coordinates.data)

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.particle_coordinates.data
                    #         ).reshape(-1)
                    # else:
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.particle_coordinates.data,
                            evalf=evalf,
                        ).reshape(-1)

                    mid_pt_coords = (
                        self.particle_coordinates.data[...]
                        + 0.5 * delta_t * v_at_Vpts / substeps
                    )

                    # validate_coords to ensure they live within the domain (or there will be trouble)

                    if restore_points_to_domain_func is not None:
                        mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                    self.particle_coordinates.data[...] = mid_pt_coords[...]

                    del mid_pt_coords

                    ## Let the swarm be updated, and then move the rest of the way

                    v_at_Vpts = np.zeros_like(self.data)

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.particle_coordinates.data
                    #         ).reshape(-1)
                    # else:
                    #
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.particle_coordinates.data,
                            evalf=evalf,
                        ).reshape(-1)

                    # if (uw.mpi.rank == 0):
                    #     print(&#34;Re-launch from X0&#34;, flush=True)

                    new_coords = X0.data[...] + delta_t * v_at_Vpts / substeps

                    # validate_coords to ensure they live within the domain (or there will be trouble)
                    if restore_points_to_domain_func is not None:
                        new_coords = restore_points_to_domain_func(new_coords)

                    self.particle_coordinates.data[...] = new_coords[...]

                    del new_coords
                    del v_at_Vpts

            # forward Euler (1st order)
            else:
                with self.access(self.particle_coordinates):
                    v_at_Vpts = np.zeros_like(self.data)

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.data
                    #         ).reshape(-1)
                    # else:
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.data,
                            evalf=evalf,
                        ).reshape(-1)

                    new_coords = self.data + delta_t * v_at_Vpts / substeps

                    # validate_coords to ensure they live within the domain (or there will be trouble)

                    if restore_points_to_domain_func is not None:
                        new_coords = restore_points_to_domain_func(new_coords)

                    self.data[...] = new_coords[...].copy()

        ## End of substepping loop

        ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
        ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
        ## original location.

        if self.recycle_rate &gt; 1:
            # Restore particles which have cycle == cycle rate (use &gt;= just in case)

            # Remove remesh points and recreate a new set at the mesh-local
            # locations that we already have stored.

            with self.access(self.particle_coordinates, self._remeshed):
                remeshed = self._remeshed.data[:, 0] == 0
                # This is one way to do it ... we can do this better though
                self.data[remeshed, 0] = 1.0e100

            swarm_size = self.dm.getLocalSize()

            num_remeshed_points = self.mesh.particle_X_orig.shape[0]

            self.dm.addNPoints(num_remeshed_points)

            cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
            rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

            # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
            # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
            # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

            perturbation = 0.00001 * (
                (0.33 / (1 + self.fill_param))
                * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
                * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
            )

            coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
            cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
            rmsh[swarm_size::] = 0

            self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

            # when we let this go, the particles may be re-distributed to
            # other processors, and we will need to rebuild the remeshed
            # array before trying to compute / assign values to variables

            for swarmVar in self.vars.values():
                if swarmVar._rebuild_on_cycle:
                    with self.access(swarmVar):
                        if swarmVar.dtype is int:
                            nnn = 1
                        else:
                            nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                        interpolated_values = (
                            swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                            #     swarmVar._meshVar.fn, self.mesh.particle_X_orig
                            # )
                        ).astype(swarmVar.dtype)

                        swarmVar.data[swarm_size::] = interpolated_values

            self.dm.migrate(remove_sent_points=True)

            with self.access(self._remeshed):
                self._remeshed.data[...] = np.mod(
                    self._remeshed.data[...] - 1, self.recycle_rate
                )

            self.cycle += 1

        return

    @timing.routine_timer_decorator
    def estimate_dt(self, V_fn):
        &#34;&#34;&#34;
        Calculates an appropriate advective timestep for the given
        mesh and velocity configuration.
        &#34;&#34;&#34;
        # we&#39;ll want to do this on an element by element basis
        # for more general mesh

        # first let&#39;s extract a max global velocity magnitude
        import math

        with self.access():
            vel = uw.function.evaluate(V_fn, self.particle_coordinates.data, evalf=True)
            try:
                magvel_squared = vel[:, 0] ** 2 + vel[:, 1] ** 2
                if self.mesh.dim == 3:
                    magvel_squared += vel[:, 2] ** 2

                max_magvel = math.sqrt(magvel_squared.max())

            except (ValueError, IndexError):
                max_magvel = 0.0

        from mpi4py import MPI

        max_magvel_glob = comm.allreduce(max_magvel, op=MPI.MAX)

        min_dx = self.mesh.get_min_radius()

        # The assumption should be that we cross one or two elements (2-4 radii), not more,
        # in a single step (order 2, means one element per half-step or something
        # that we can broadly interpret that way)

        if max_magvel_glob != 0.0:
            return min_dx / max_magvel_glob
        else:
            return None</code></pre>
</details>
<div class="desc"><p>Particle swarm implementation with automatic mesh-particle interactions.</p>
<p>The <code><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></code> class is Underworld's primary particle management system, built on PETSc's
DMSWARM_PIC type. It provides automatic particle migration, mesh-particle connectivity,
and streamlined particle operations for Lagrangian particle tracking and data storage.</p>
<p>Differences from UW Swarm:
- <strong>Mesh Integration</strong>: Built-in particle-in-cell (PIC) connectivity with automatic cell tracking
- <strong>Migration</strong>: Uses the standard PETSc strategy for migration which depends on the DM type. This requires
calculation of cell-relationships each time the coordinates are updated and particles that are not found will
be deleted.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mesh</code></strong> :&ensp;<code>uw.discretisation.Mesh</code></dt>
<dd>The mesh object that defines the computational domain. Particles will be
automatically associated with mesh cells for efficient spatial operations.</dd>
<dt><strong><code>recycle_rate</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Rate at which particles are recycled for streak management. If &gt; 1, enables
streak particle functionality where particles are duplicated and tracked
across multiple cycles. Default is 0 (no recycling).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Enable verbose output for debugging and monitoring particle operations.
Default is False.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>mesh</code></strong> :&ensp;<code>uw.discretisation.Mesh</code></dt>
<dd>Reference to the associated mesh object.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Spatial dimension of the mesh (2D or 3D).</dd>
<dt><strong><code>cdim</code></strong> :&ensp;<code>int</code></dt>
<dd>Coordinate dimension of the mesh.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Direct access to particle coordinate data.</dd>
<dt><strong><code>particle_coordinates</code></strong> :&ensp;<code><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></code></dt>
<dd>SwarmVariable containing particle coordinate information (auto-created).</dd>
<dt><strong><code>particle_cellid</code></strong> :&ensp;<code><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></code></dt>
<dd>SwarmVariable containing particle cell ID information (auto-created).</dd>
<dt><strong><code>recycle_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Current recycle rate for streak management.</dd>
<dt><strong><code>cycle</code></strong> :&ensp;<code>int</code></dt>
<dd>Current cycle number for streak particles.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>populate_petsc(fill_param=1)
Populate swarm using PETSc's built-in particle generation.
populate(fill_param=1, layout=SwarmPICLayout.GAUSS)
Populate the swarm with particles using specified layout.
add_particles_with_coordinates(coords)
Add new particles at specified coordinate locations.
add_variable(name, size, dtype=float)
Add a new variable to track additional particle properties.
save(filename, meshUnits=1.0, swarmUnits=1.0, units="dimensionless")
Save swarm data to file.
read_timestep(filename, step_name, outputPath="./output/")
Read swarm data from a specific timestep file.
advection(V_fn, delta_t, evalf=False, corrector=True, restore_points_func=None)
Advect particles using a velocity field with automatic migration.
estimate_dt(V_fn, dt_min=1.0e-15, dt_max=1.0)
Estimate appropriate timestep for particle advection.</p>
<h2 id="examples">Examples</h2>
<p>Create a standard swarm with automatic features:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import underworld3 as uw
&gt;&gt;&gt; mesh = uw.meshing.UnstructuredSimplexBox(minCoords=(0,0), maxCoords=(1,1))
&gt;&gt;&gt; swarm = uw.swarm.Swarm(mesh=mesh)
&gt;&gt;&gt; swarm.populate(fill_param=2, layout=uw.swarm.SwarmPICLayout.GAUSS)
</code></pre>
<p>Access automatic coordinate and cell ID fields:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; coords = swarm.particle_coordinates.data
&gt;&gt;&gt; cell_ids = swarm.particle_cellid.data
</code></pre>
<p>Create a streak swarm with recycling:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; streak_swarm = uw.swarm.Swarm(mesh=mesh, recycle_rate=5)
&gt;&gt;&gt; streak_swarm.populate(fill_param=1)
</code></pre>
<p>Add custom particle data and perform advection:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; temperature = swarm.add_variable(&quot;temperature&quot;, 1)
&gt;&gt;&gt; velocity_field = mesh.add_variable(&quot;velocity&quot;, mesh.dim)
&gt;&gt;&gt; # ... set up velocity field ...
&gt;&gt;&gt; swarm.advection(velocity_field.sym, delta_t=0.01)  # Automatic migration
</code></pre>
<h2 id="notes">Notes</h2>
<ul>
<li>Particle migration occurs automatically during advection operations</li>
<li>Coordinate and cell ID fields are created and managed automatically at the
PETSc level</li>
</ul></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.NodalPointPICSwarm" href="#underworld3.swarm.NodalPointPICSwarm">NodalPointPICSwarm</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.PICSwarm.instances"><code class="name">var <span class="ident">instances</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.PICSwarm.data"><code class="name">prop <span class="ident">data</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self):
    return self.particle_coordinates.data</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.mesh"><code class="name">prop <span class="ident">mesh</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mesh(self):
    return self._mesh</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.particle_cellid"><code class="name">prop <span class="ident">particle_cellid</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def particle_cellid(self):
    return self._cellid_var</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.particle_coordinates"><code class="name">prop <span class="ident">particle_coordinates</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def particle_coordinates(self):
    return self._coord_var</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.vars"><code class="name">prop <span class="ident">vars</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def vars(self):
    return self._vars</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.PICSwarm.access"><code class="name flex">
<span>def <span class="ident">access</span></span>(<span>self,<br>*writeable_vars: <a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def access(self, *writeable_vars: SwarmVariable):
    &#34;&#34;&#34;
    This context manager makes the underlying swarm variables data available to
    the user. The data should be accessed via the variables `data` handle.

    As default, all data is read-only. To enable writeable data, the user should
    specify which variable they wish to modify.

    At the conclusion of the users context managed block, numerous further operations
    will be automatically executed. This includes swarm parallel migration routines
    where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
    variable proxy mesh variables will also be updated for modifed swarm variables.

    Parameters
    ----------
    writeable_vars
        The variables for which data write access is required.

    Example
    -------

    &gt;&gt;&gt; import underworld3 as uw
    &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
    &gt;&gt;&gt; with someMesh.deform_mesh():
    ...     someMesh.data[0] = [0.1,0.1]
    &gt;&gt;&gt; someMesh.data[0]
    array([ 0.1,  0.1])
    &#34;&#34;&#34;
    import time

    uw.timing._incrementDepth()
    stime = time.time()

    deaccess_list = []
    for var in self._vars.values():
        # if already accessed within higher level context manager, continue.
        if var._is_accessed == True:
            continue
        # set flag so variable status can be known elsewhere
        var._is_accessed = True
        # add to de-access list to rewind this later
        deaccess_list.append(var)
        # grab numpy object, setting read only if necessary
        var._data = self.dm.getField(var.clean_name).reshape(
            (-1, var.num_components)
        )
        assert var._data is not None
        if var not in writeable_vars:
            var._old_data_flag = var._data.flags.writeable
            var._data.flags.writeable = False
        else:
            # increment variable state
            var._increment()

        # make view for each var component
        if var._proxy:
            for i in range(0, var.shape[0]):
                for j in range(0, var.shape[1]):
                    var._data_container[i, j] = var._data_container[i, j]._replace(
                        data=var.data[:, var._data_layout(i, j)],
                    )

    # if particles moving, update swarm state
    if self.particle_coordinates in writeable_vars:
        self._increment()

    # Create a class which specifies the required context
    # manager hooks (`__enter__`, `__exit__`).
    class exit_manager:
        def __init__(self, swarm):
            self.em_swarm = swarm

        def __enter__(self):

            pass

        def __exit__(self, *args):

            for var in self.em_swarm.vars.values():
                # only de-access variables we have set access for.
                if var not in deaccess_list:
                    continue
                # set this back, although possibly not required.
                if var not in writeable_vars:
                    var._data.flags.writeable = var._old_data_flag
                var._data = None
                self.em_swarm.dm.restoreField(var.clean_name)
                var._is_accessed = False
            # do particle migration if coords changes

            if self.em_swarm.particle_coordinates in writeable_vars:
                # let&#39;s use the mesh index to update the particles owning cells.
                # note that the `petsc4py` interface is more convenient here as the
                # `SwarmVariable.data` interface is controlled by the context manager
                # that we are currently within, and it is therefore too easy to
                # get things wrong that way.

                cellid = self.em_swarm.dm.getField(&#34;DMSwarm_cellid&#34;)
                coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                    (-1, self.em_swarm.dim)
                )

                cellid[:] = self.em_swarm.mesh.get_closest_cells(coords).reshape(-1)

                # num_lost = np.where(cellid == -1)[0].shape[0]
                # print(
                #     f&#34;{uw.mpi.rank} - EM 1: illegal_cells - {num_lost}&#34;, flush=True
                # )

                # if num_lost != 0:
                #     print(&#34;LOST: &#34;, coords[np.where(cellid == -1)])

                self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                self.em_swarm.dm.restoreField(&#34;DMSwarm_cellid&#34;)
                # now migrate.

                self.em_swarm.dm.migrate(remove_sent_points=True)

                # void these things too
                self.em_swarm._index = None
                self.em_swarm._nnmapdict = {}

            # do var updates
            for var in self.em_swarm.vars.values():
                # if swarm migrated, update all.
                # if var updated, update var.
                if (self.em_swarm.particle_coordinates in writeable_vars) or (
                    var in writeable_vars
                ):
                    var._update()

                if var._proxy:
                    for i in range(0, var.shape[0]):
                        for j in range(0, var.shape[1]):
                            # var._data_ij[i, j] = None
                            var._data_container[i, j] = var._data_container[
                                i, j
                            ]._replace(
                                data=f&#34;SwarmVariable[...].data is only available within mesh.access() context&#34;,
                            )

            uw.timing._decrementDepth()
            uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

    return exit_manager(self)</code></pre>
</details>
<div class="desc"><p>This context manager makes the underlying swarm variables data available to
the user. The data should be accessed via the variables <code>data</code> handle.</p>
<p>As default, all data is read-only. To enable writeable data, the user should
specify which variable they wish to modify.</p>
<p>At the conclusion of the users context managed block, numerous further operations
will be automatically executed. This includes swarm parallel migration routines
where the swarm's <code>particle_coordinates</code> variable has been modified. The swarm
variable proxy mesh variables will also be updated for modifed swarm variables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>writeable_vars</code></strong></dt>
<dd>The variables for which data write access is required.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import underworld3 as uw
&gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
&gt;&gt;&gt; with someMesh.deform_mesh():
...     someMesh.data[0] = [0.1,0.1]
&gt;&gt;&gt; someMesh.data[0]
array([ 0.1,  0.1])
</code></pre></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.add_particles_with_coordinates"><code class="name flex">
<span>def <span class="ident">add_particles_with_coordinates</span></span>(<span>self, coordinatesArray) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_particles_with_coordinates(self, coordinatesArray) -&gt; int:
    &#34;&#34;&#34;
    Add particles to the swarm using particle coordinates provided
    using a numpy array.

    Note that particles with coordinates NOT local to the current processor will
    be rejected / ignored.

    Either include an array with all coordinates to all processors
    or an array with the local coordinates.

    Parameters
    ----------
    coordinatesArray : numpy.ndarray
        The numpy array containing the coordinate of the new particles. Array is
        expected to take shape n*dim, where n is the number of new particles, and
        dim is the dimensionality of the swarm&#39;s supporting mesh.

    Returns
    --------
    npoints: int
        The number of points added to the local section of the swarm.
    &#34;&#34;&#34;

    if not isinstance(coordinatesArray, np.ndarray):
        raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
    if not len(coordinatesArray.shape) == 2:
        raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
    if not coordinatesArray.shape[1] == self.mesh.dim:
        #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
        raise ValueError(
            &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                          number of particles to add, and &#39;dim&#39; is the dimensionality of
                          the supporting mesh ({}).&#34;&#34;&#34;.format(
                self.mesh.dim
            )
        )

    cells = self.mesh.get_closest_local_cells(coordinatesArray)

    valid_coordinates = coordinatesArray[cells != -1]
    valid_cells = cells[cells != -1]

    npoints = len(valid_coordinates)
    swarm_size = self.dm.getLocalSize()

    # -1 means no particles have been added yet
    if swarm_size == -1:
        swarm_size = 0
        npoints = npoints + 1

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(npoints=npoints)

    cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

    coords[swarm_size::, :] = valid_coordinates[:, :]
    cellid[swarm_size::] = valid_cells[:]

    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
    self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    # Here we update the swarm cycle values as required

    if self.recycle_rate &gt; 1:
        with self.access(self._remeshed):
            # self._Xorig.data[...] = coordinatesArray
            self._remeshed.data[...] = 0

    self.dm.migrate(remove_sent_points=True)

    return npoints</code></pre>
</details>
<div class="desc"><p>Add particles to the swarm using particle coordinates provided
using a numpy array.</p>
<p>Note that particles with coordinates NOT local to the current processor will
be rejected / ignored.</p>
<p>Either include an array with all coordinates to all processors
or an array with the local coordinates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coordinatesArray</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The numpy array containing the coordinate of the new particles. Array is
expected to take shape n*dim, where n is the number of new particles, and
dim is the dimensionality of the swarm's supporting mesh.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>npoints</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of points added to the local section of the swarm.</dd>
</dl></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.add_variable"><code class="name flex">
<span>def <span class="ident">add_variable</span></span>(<span>self, name, size=1, dtype=builtins.float, proxy_degree=2)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_variable(
    self,
    name,
    size=1,
    dtype=float,
    proxy_degree=2,
    _nn_proxy=False,
):
    return SwarmVariable(
        name,
        self,
        size,
        dtype=dtype,
        proxy_degree=proxy_degree,
        _nn_proxy=_nn_proxy,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.advection"><code class="name flex">
<span>def <span class="ident">advection</span></span>(<span>self,<br>V_fn,<br>delta_t,<br>order=2,<br>corrector=False,<br>restore_points_to_domain_func=None,<br>evalf=False,<br>step_limit=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def advection(
    self,
    V_fn,
    delta_t,
    order=2,
    corrector=False,
    restore_points_to_domain_func=None,
    evalf=False,
    step_limit=True,
):

    dt_limit = self.estimate_dt(V_fn)

    if step_limit and dt_limit is not None:
        substeps = int(max(1, round(abs(delta_t) / dt_limit)))
    else:
        substeps = 1

    if uw.mpi.rank == 0 and self.verbose:
        print(f&#34;Substepping {substeps} / {abs(delta_t) / dt_limit}, {delta_t} &#34;)

    # X0 holds the particle location at the start of advection
    # This is needed because the particles may be migrated off-proc
    # during timestepping.

    X0 = self._X0

    V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

    # Use current velocity to estimate where the particles would have
    # landed in an implicit step. WE CANT DO THIS WITH SUB-STEPPING unless
    # We have a lot more information about the previous launch point / timestep
    # Also: how does this interact with the particle restoration function ?

    # if corrector == True and not self._X0_uninitialised:
    #     with self.access(self.particle_coordinates):
    #         v_at_Vpts = np.zeros_like(self.data)

    #         if evalf:
    #             for d in range(self.dim):
    #                 v_at_Vpts[:, d] = uw.function.evalf(
    #                     V_fn_matrix[d], self.data
    #                 ).reshape(-1)
    #         else:
    #             for d in range(self.dim):
    #                 v_at_Vpts[:, d] = uw.function.evaluate(
    #                     V_fn_matrix[d], self.data
    #                 ).reshape(-1)

    #         corrected_position = X0.data.copy() + delta_t * v_at_Vpts
    #         if restore_points_to_domain_func is not None:
    #             corrected_position = restore_points_to_domain_func(
    #                 corrected_position
    #             )

    #         updated_current_coords = 0.5 * (corrected_position + self.data.copy())

    #         # validate_coords to ensure they live within the domain (or there will be trouble)

    #         if restore_points_to_domain_func is not None:
    #             updated_current_coords = restore_points_to_domain_func(
    #                 updated_current_coords
    #             )

    #         self.data[...] = updated_current_coords[...]

    #         del updated_current_coords
    #         del v_at_Vpts

    print(f&#34;{ uw.mpi.rank}: Peace&#34;, flush=True)

    # Wrap this whole thing in sub-stepping loop
    for step in range(0, substeps):

        with self.access(X0):
            X0.data[...] = self.particle_coordinates.data[...]

        # Mid point algorithm (2nd order)

        if order == 2:
            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.particle_coordinates.data)

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.particle_coordinates.data
                #         ).reshape(-1)
                # else:
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.particle_coordinates.data,
                        evalf=evalf,
                    ).reshape(-1)

                mid_pt_coords = (
                    self.particle_coordinates.data[...]
                    + 0.5 * delta_t * v_at_Vpts / substeps
                )

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                self.particle_coordinates.data[...] = mid_pt_coords[...]

                del mid_pt_coords

                ## Let the swarm be updated, and then move the rest of the way

                v_at_Vpts = np.zeros_like(self.data)

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.particle_coordinates.data
                #         ).reshape(-1)
                # else:
                #
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.particle_coordinates.data,
                        evalf=evalf,
                    ).reshape(-1)

                # if (uw.mpi.rank == 0):
                #     print(&#34;Re-launch from X0&#34;, flush=True)

                new_coords = X0.data[...] + delta_t * v_at_Vpts / substeps

                # validate_coords to ensure they live within the domain (or there will be trouble)
                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.particle_coordinates.data[...] = new_coords[...]

                del new_coords
                del v_at_Vpts

        # forward Euler (1st order)
        else:
            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.data)

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.data
                #         ).reshape(-1)
                # else:
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.data,
                        evalf=evalf,
                    ).reshape(-1)

                new_coords = self.data + delta_t * v_at_Vpts / substeps

                # validate_coords to ensure they live within the domain (or there will be trouble)

                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords[...].copy()

    ## End of substepping loop

    ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
    ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
    ## original location.

    if self.recycle_rate &gt; 1:
        # Restore particles which have cycle == cycle rate (use &gt;= just in case)

        # Remove remesh points and recreate a new set at the mesh-local
        # locations that we already have stored.

        with self.access(self.particle_coordinates, self._remeshed):
            remeshed = self._remeshed.data[:, 0] == 0
            # This is one way to do it ... we can do this better though
            self.data[remeshed, 0] = 1.0e100

        swarm_size = self.dm.getLocalSize()

        num_remeshed_points = self.mesh.particle_X_orig.shape[0]

        self.dm.addNPoints(num_remeshed_points)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

        # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
        # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
        # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

        perturbation = 0.00001 * (
            (0.33 / (1 + self.fill_param))
            * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
            * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
        )

        coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
        cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
        rmsh[swarm_size::] = 0

        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

        # when we let this go, the particles may be re-distributed to
        # other processors, and we will need to rebuild the remeshed
        # array before trying to compute / assign values to variables

        for swarmVar in self.vars.values():
            if swarmVar._rebuild_on_cycle:
                with self.access(swarmVar):
                    if swarmVar.dtype is int:
                        nnn = 1
                    else:
                        nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                    interpolated_values = (
                        swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                        #     swarmVar._meshVar.fn, self.mesh.particle_X_orig
                        # )
                    ).astype(swarmVar.dtype)

                    swarmVar.data[swarm_size::] = interpolated_values

        self.dm.migrate(remove_sent_points=True)

        with self.access(self._remeshed):
            self._remeshed.data[...] = np.mod(
                self._remeshed.data[...] - 1, self.recycle_rate
            )

        self.cycle += 1

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.estimate_dt"><code class="name flex">
<span>def <span class="ident">estimate_dt</span></span>(<span>self, V_fn)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def estimate_dt(self, V_fn):
    &#34;&#34;&#34;
    Calculates an appropriate advective timestep for the given
    mesh and velocity configuration.
    &#34;&#34;&#34;
    # we&#39;ll want to do this on an element by element basis
    # for more general mesh

    # first let&#39;s extract a max global velocity magnitude
    import math

    with self.access():
        vel = uw.function.evaluate(V_fn, self.particle_coordinates.data, evalf=True)
        try:
            magvel_squared = vel[:, 0] ** 2 + vel[:, 1] ** 2
            if self.mesh.dim == 3:
                magvel_squared += vel[:, 2] ** 2

            max_magvel = math.sqrt(magvel_squared.max())

        except (ValueError, IndexError):
            max_magvel = 0.0

    from mpi4py import MPI

    max_magvel_glob = comm.allreduce(max_magvel, op=MPI.MAX)

    min_dx = self.mesh.get_min_radius()

    # The assumption should be that we cross one or two elements (2-4 radii), not more,
    # in a single step (order 2, means one element per half-step or something
    # that we can broadly interpret that way)

    if max_magvel_glob != 0.0:
        return min_dx / max_magvel_glob
    else:
        return None</code></pre>
</details>
<div class="desc"><p>Calculates an appropriate advective timestep for the given
mesh and velocity configuration.</p></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.petsc_save_checkpoint"><code class="name flex">
<span>def <span class="ident">petsc_save_checkpoint</span></span>(<span>self, swarmName: str, index: int, outputPath: str | None = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def petsc_save_checkpoint(
    self,
    swarmName: str,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
):
    &#34;&#34;&#34;

    Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    &#34;&#34;&#34;

    x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:05d}.xmf&#34;
    self.dm.viewXDMF(x_swarm_fname)</code></pre>
</details>
<div class="desc"><p>Use PETSc to save the swarm and attached data to a .pbin and xdmf file.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.</p></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.populate"><code class="name flex">
<span>def <span class="ident">populate</span></span>(<span>self, fill_param: int | None = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def populate(
    self,
    fill_param: Optional[int] = 1,
):
    &#34;&#34;&#34;
    Populate the swarm with particles throughout the domain.

    Parameters
    ----------
    fill_param:
        Parameter determining the particle count per cell (per dimension)
        for the given layout, using the mesh degree.

    cell_search:
        Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)

    &#34;&#34;&#34;

    self.fill_param = fill_param

    newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
    newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

    if np.any(newp_cells0 &gt; self.mesh._centroids.shape[0]):
        raise RuntimeError(&#34;Some new coordinates can&#39;t find a owning cell - Error&#34;)

    # valid = newp_cells0 != -1
    # newp_coords = newp_coords0[valid]
    # newp_cells = newp_cells0[valid]
    newp_coords = newp_coords0
    newp_cells = newp_cells0

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(newp_coords.shape[0] + 1)

    cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

    coords[...] = newp_coords[...]
    cellid[:] = newp_cells[:]

    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
    self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    ## Now make a series of copies to allow the swarm cycling to
    ## work correctly (if required)

    # cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
    # lost = np.where(cellid == -1)
    # print(f&#34;{uw.mpi.rank} - lost particles: {lost[0].shape} out of {cellid.shape}&#34;, flush=True)
    # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

    if self.recycle_rate &gt; 1:
        with self.access():
            # This is a mesh-local quantity, so let&#39;s just
            # store it on the mesh in an ad_hoc fashion for now

            self.mesh.particle_X_orig = self.particle_coordinates.data.copy()
            self.mesh.particle_CellID_orig = self._cellid_var.data.copy()

        with self.access():
            swarm_orig_size = self.particle_coordinates.data.shape[0]
            all_local_coords = np.vstack(
                (self.particle_coordinates.data,) * (self.recycle_rate)
            )
            all_local_cells = np.vstack(
                (self._cellid_var.data,) * (self.recycle_rate)
            )

            swarm_new_size = all_local_coords.data.shape[0]

        self.dm.addNPoints(swarm_new_size - swarm_orig_size)

        cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = (
            all_local_coords[...]
            + (0.33 / (1 + fill_param))
            * (np.random.random(size=all_local_coords.shape) - 0.5)
            * 0.00001
            * self.mesh._search_lengths[all_local_cells]  # typical cell size
        )
        cellid[:] = all_local_cells[:, 0]

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_cellid&#34;)

        ## Now set the cycle values

        with self.access(self._remeshed):
            for i in range(0, self.recycle_rate):
                offset = swarm_orig_size * i
                self._remeshed.data[offset::, 0] = i

    return</code></pre>
</details>
<div class="desc"><p>Populate the swarm with particles throughout the domain.</p>
<h2 id="parameters">Parameters</h2>
<p>fill_param:
Parameter determining the particle count per cell (per dimension)
for the given layout, using the mesh degree.</p>
<p>cell_search:
Use k-d tree to locate nearest cells (fails if this swarm is used to build a k-d tree)</p></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.populate_petsc"><code class="name flex">
<span>def <span class="ident">populate_petsc</span></span>(<span>self,<br>fill_param: int | None = 3,<br>layout: <a title="underworld3.swarm.SwarmPICLayout" href="#underworld3.swarm.SwarmPICLayout">SwarmPICLayout</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def populate_petsc(
    self,
    fill_param: Optional[int] = 3,
    layout: Optional[SwarmPICLayout] = None,
):
    &#34;&#34;&#34;
    Populate the swarm with particles throughout the domain.

    When using SwarmPICLayout.REGULAR,     `fill_param` defines the number of points in each spatial direction.
    When using SwarmPICLayout.GAUSS,       `fill_param` defines the number of quadrature points in each spatial direction.
    When using SwarmPICLayout.SUBDIVISION, `fill_param` defines the number times the reference cell is sub-divided.

    Parameters
    ----------
    fill_param:
        Parameter determining the particle count per cell for the given layout.
    layout:
        Type of layout to use. Defaults to `SwarmPICLayout.REGULAR` for mesh objects with simplex
        type cells, and `SwarmPICLayout.GAUSS` otherwise.

    &#34;&#34;&#34;

    self.fill_param = fill_param

    &#34;&#34;&#34;
    Currently (2021.11.15) supported by PETSc release 3.16.x

    When using a DMPLEX the following case are supported:
          (i) DMSWARMPIC_LAYOUT_REGULAR: 2D (triangle),
         (ii) DMSWARMPIC_LAYOUT_GAUSS: 2D and 3D provided the cell is a tri/tet or a quad/hex,
        (iii) DMSWARMPIC_LAYOUT_SUBDIVISION: 2D and 3D for quad/hex and 2D tri.

    So this means, simplex mesh in 3D only supports GAUSS - This is based
    on the tensor product locations so it is not uniform in the cells.
    &#34;&#34;&#34;

    if layout == None:
        layout = SwarmPICLayout.GAUSS

    if not isinstance(layout, SwarmPICLayout):
        raise ValueError(&#34;&#39;layout&#39; must be an instance of &#39;SwarmPICLayout&#39;&#34;)

    self.layout = layout
    self.dm.finalizeFieldRegister()

    ## Commenting this out for now.
    ## Code seems to operate fine without it, and the
    ## existing values are wrong. It should be something like
    ## `(elend-elstart)*fill_param^dim` for quads, and around
    ## half that for simplices, depending on layout.
    # elstart,elend = self.mesh.dm.getHeightStratum(0)
    # self.dm.setLocalSizes((elend-elstart) * fill_param, 0)

    self.dm.insertPointUsingCellDM(self.layout.value, fill_param)
    return</code></pre>
</details>
<div class="desc"><p>Populate the swarm with particles throughout the domain.</p>
<p>When using SwarmPICLayout.REGULAR,
<code>fill_param</code> defines the number of points in each spatial direction.
When using SwarmPICLayout.GAUSS,
<code>fill_param</code> defines the number of quadrature points in each spatial direction.
When using SwarmPICLayout.SUBDIVISION, <code>fill_param</code> defines the number times the reference cell is sub-divided.</p>
<h2 id="parameters">Parameters</h2>
<p>fill_param:
Parameter determining the particle count per cell for the given layout.
layout:
Type of layout to use. Defaults to <code><a title="underworld3.swarm.SwarmPICLayout.REGULAR" href="#underworld3.swarm.SwarmPICLayout.REGULAR">SwarmPICLayout.REGULAR</a></code> for mesh objects with simplex
type cells, and <code><a title="underworld3.swarm.SwarmPICLayout.GAUSS" href="#underworld3.swarm.SwarmPICLayout.GAUSS">SwarmPICLayout.GAUSS</a></code> otherwise.</p></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.read_timestep"><code class="name flex">
<span>def <span class="ident">read_timestep</span></span>(<span>self, base_filename: str, swarm_id: str, index: int, outputPath: str | None = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def read_timestep(
    self,
    base_filename: str,
    swarm_id: str,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
):
    output_base_name = os.path.join(outputPath, base_filename)
    swarm_file = output_base_name + f&#34;.{swarm_id}.{index:05}.h5&#34;

    ### open up file with coords on all procs
    with h5py.File(f&#34;{swarm_file}&#34;, &#34;r&#34;) as h5f:
        coordinates = h5f[&#34;coordinates&#34;][:]

    #### utilises the UW function for adding a swarm by an array
    self.add_particles_with_coordinates(coordinates)

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self,<br>filename: int,<br>compression: bool | None = False,<br>compressionType: str | None = 'gzip',<br>force_sequential=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save(
    self,
    filename: int,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential=False,
):
    &#34;&#34;&#34;

    Save the swarm coordinates to a h5 file.

    Parameters
    ----------
    filename :
        The filename of the swarm checkpoint file to save to disk.
    compression :
        Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
    compressionType :
        Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



    &#34;&#34;&#34;
    if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
        warnings.warn(
            &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
            stacklevel=2,
        )
    if filename.endswith(&#34;.h5&#34;) == False:
        raise RuntimeError(&#34;The filename must end with .h5&#34;)
    if compression == True and comm.rank == 0:
        warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

    if h5py.h5.get_config().mpi == True and not force_sequential:
        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores. This is probably why the parallel
        # h5py write hangs

        with self.access():
            data_copy = self.data[:].copy()

        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
            if compression == True:
                h5f.create_dataset(
                    &#34;coordinates&#34;,
                    data=data_copy[:],
                    compression=compressionType,
                )
            else:
                h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])

        del data_copy

    else:
        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores

        with self.access():
            data_copy = self.data[:].copy()

        if comm.rank == 0:
            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                    )

        comm.barrier()
        for i in range(1, comm.size):
            if comm.rank == i:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                    h5f[&#34;coordinates&#34;].resize(
                        (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                        axis=0,
                    )
                    # passive swarm, zero local particles is not unusual
                    if data_copy.shape[0] &gt; 0:
                        h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
            comm.barrier()
        comm.barrier()

        del data_copy

    return</code></pre>
</details>
<div class="desc"><p>Save the swarm coordinates to a h5 file.</p>
<h2 id="parameters">Parameters</h2>
<p>filename :
The filename of the swarm checkpoint file to save to disk.
compression :
Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
compressionType :
Type of compression to use, 'gzip' and 'lzf' supported. 'gzip' is default. Compression also needs to be set to 'True'.</p></div>
</dd>
<dt id="underworld3.swarm.PICSwarm.write_timestep"><code class="name flex">
<span>def <span class="ident">write_timestep</span></span>(<span>self,<br>filename: str,<br>swarmname: str,<br>index: int,<br>swarmVars: list | None = None,<br>outputPath: str | None = '',<br>time: int | None = None,<br>compression: bool | None = False,<br>compressionType: str | None = 'gzip',<br>force_sequential: bool | None = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def write_timestep(
    self,
    filename: str,
    swarmname: str,
    index: int,
    swarmVars: Optional[list] = None,
    outputPath: Optional[str] = &#34;&#34;,
    time: Optional[int] = None,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential: Optional[bool] = False,
):
    &#34;&#34;&#34;

    Save data to h5 and a corresponding xdmf for visualisation using h5py.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    swarmVars :
        List of swarm objects to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    time :
        Attach the time to the generated xdmf.
    compression :
        Whether to compress the h5 files [bool].
    compressionType :
        The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
    &#34;&#34;&#34;

    # This will eliminate the issue of whether or not to put path separators in the
    # outputPath. Also does the right thing if outputPath is &#34;&#34;

    output_base_name = os.path.join(outputPath, filename) + &#34;.&#34; + swarmname

    # check the directory where we will write checkpoint
    dir_path = os.path.dirname(output_base_name)  # get directory

    # check if path exists
    if os.path.exists(os.path.abspath(dir_path)):  # easier to debug abs
        pass
    else:
        raise RuntimeError(f&#34;{os.path.abspath(dir_path)} does not exist&#34;)

    # check if we have write access
    if os.access(os.path.abspath(dir_path), os.W_OK):
        pass
    else:
        raise RuntimeError(f&#34;No write access to {os.path.abspath(dir_path)}&#34;)

    # could also try to coerce this to be a list and raise if it fails (tuple, singleton ... )
    # also ... why the typechecking if this can still happen

    if swarmVars is not None and not isinstance(swarmVars, list):
        raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

    else:
        ### save the swarm particle location
        self.save(
            filename=f&#34;{output_base_name}.{index:05d}.h5&#34;,
            compression=compression,
            compressionType=compressionType,
            force_sequential=force_sequential,
        )

    #### Generate a h5 file for each field
    if swarmVars != None:
        for field in swarmVars:
            field.save(
                filename=f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

    if uw.mpi.rank == 0:
        ### only need to combine the h5 files to a single xdmf on one proc
        with open(f&#34;{output_base_name}.{index:05d}.xdmf&#34;, &#34;w&#34;) as xdmf:
            # Write the XDMF header
            xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
            xdmf.write(
                &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
            )
            xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
            xdmf.write(
                f&#39;&lt;Grid Name=&#34;{output_base_name}.{index:05d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
            )

            if time != None:
                xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

            # Write the grid element for the HDF5 dataset
            with h5py.File(f&#34;{output_base_name}.{index:05}.h5&#34;, &#34;r&#34;) as h5f:
                xdmf.write(
                    f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                )
                if h5f[&#34;coordinates&#34;].shape[1] == 2:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                xdmf.write(
                    f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                )
                xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

            # Write the attribute element for the field
            if swarmVars != None:
                for field in swarmVars:
                    with h5py.File(
                        f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;, &#34;r&#34;
                    ) as h5f:
                        if h5f[&#34;data&#34;].dtype == np.int32:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 1:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        else:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )

                        xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
            else:
                pass

            # Write the XDMF footer
            xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)</code></pre>
</details>
<div class="desc"><p>Save data to h5 and a corresponding xdmf for visualisation using h5py.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
swarmVars :
List of swarm objects to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.
time :
Attach the time to the generated xdmf.
compression :
Whether to compress the h5 files [bool].
compressionType :
The type of compression to use. 'gzip' and 'lzf' are the supported types, with 'gzip' as the default.</p></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.Swarm"><code class="flex name class">
<span>class <span class="ident">Swarm</span></span>
<span>(</span><span>mesh, recycle_rate=0, verbose=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Swarm(Stateful, uw_object):
    &#34;&#34;&#34;
    A basic particle swarm implementation for Lagrangian particle tracking and data storage.

    The UW `Swarm` class provides a simplified particle management system that uses
    PETSc&#39;s DMSWARM_BASIC type. Unlike the standard `Swarm` class, this implementation
    does not rely on PETSc to determine ranks for particle migration but instead uses
    our own kdtree neighbour-domain computations.

    This class is preferred for most operations except where particle / cell relationships
    are always required.

    Parameters
    ----------
    mesh : uw.discretisation.Mesh
        The mesh object that defines the computational domain for particle operations.
        Particles will be associated with this mesh for spatial queries and operations.
    recycle_rate : int, optional
        Rate at which particles are recycled for streak management. If &gt; 1, enables
        streak particle functionality where particles are duplicated and tracked
        across multiple cycles. Default is 0 (no recycling).
    verbose : bool, optional
        Enable verbose output for debugging and monitoring particle operations.
        Default is False.

    Attributes
    ----------
    mesh : uw.discretisation.Mesh
        Reference to the associated mesh object.
    dim : int
        Spatial dimension of the mesh (2D or 3D).
    cdim : int
        Coordinate dimension of the mesh.
    data : numpy.ndarray
        Direct access to particle coordinate data.
    particle_coordinates : SwarmVariable
        SwarmVariable containing particle coordinate information.
    recycle_rate : int
        Current recycle rate for streak management.
    cycle : int
        Current cycle number for streak particles.

    Methods
    -------
    populate(fill_param=1)
        Populate the swarm with particles throughout the domain.
    migrate(remove_sent_points=True, delete_lost_points=True, max_its=10)
        Manually migrate particles across MPI processes after coordinate updates.
    add_particles_with_coordinates(coords)
        Add new particles at specified coordinate locations.
    add_particles_with_global_coordinates(coords)
        Add particles using global coordinate system.
    add_variable(name, size, dtype=float)
        Add a new variable to track additional particle properties.
    save(filename, meshUnits=1.0, swarmUnits=1.0, units=&#34;dimensionless&#34;)
        Save swarm data to file.
    read_timestep(filename, step_name, outputPath=&#34;./output/&#34;)
        Read swarm data from a specific timestep file.
    advection(V_fn, delta_t, evalf=False, corrector=True, restore_points_func=None)
        Advect particles using a velocity field.
    estimate_dt(V_fn, dt_min=1.0e-15, dt_max=1.0)
        Estimate appropriate timestep for particle advection.

    Examples
    --------
    Create a basic swarm and populate with particles:

    &gt;&gt;&gt; import underworld3 as uw
    &gt;&gt;&gt; mesh = uw.meshing.UnstructuredSimplexBox(minCoords=(0,0), maxCoords=(1,1))
    &gt;&gt;&gt; swarm = uw.swarm.Swarm(mesh=mesh)
    &gt;&gt;&gt; swarm.populate(fill_param=2)

    Create a streak swarm with recycling:

    &gt;&gt;&gt; streak_swarm = uw.swarm.Swarm(mesh=mesh, recycle_rate=5)
    &gt;&gt;&gt; streak_swarm.populate(fill_param=1)

    Add custom particle data:

    &gt;&gt;&gt; temperature = swarm.add_variable(&#34;temperature&#34;, 1)
    &gt;&gt;&gt; velocity = swarm.add_variable(&#34;velocity&#34;, mesh.dim)

    Manual particle migration after coordinate updates:

    Note: particle migration is still called automatically when we
    `access` and update the particle_coordinates variables

    Note: `swarm.populate` uses a the mesh point locations for discontinuous interpolants to
    determine the particle locations.

    &#34;&#34;&#34;

    instances = 0

    @timing.routine_timer_decorator
    def __init__(self, mesh, recycle_rate=0, verbose=False):
        Swarm.instances += 1

        self.verbose = verbose
        self._mesh = mesh
        self.dim = mesh.dim
        self.cdim = mesh.cdim
        self.dm = PETSc.DMSwarm().create()
        self.dm.setDimension(self.dim)
        self.dm.setType(SwarmType.DMSWARM_BASIC.value)
        self._data = None

        # Is the swarm a streak-swarm ?
        self.recycle_rate = recycle_rate
        self.cycle = 0

        # dictionary for variables

        # import weakref (not helpful as garbage collection does not remove the fields from the DM)
        # self._vars = weakref.WeakValueDictionary()
        self._vars = {}

        # add variable to handle particle coords - match name from PIC_Swarm for consistency
        self._coord_var = SwarmVariable(
            &#34;DMSwarmPIC_coor&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=True,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # add variable to hold swarm coordinates during position updates
        self._X0 = uw.swarm.SwarmVariable(
            &#34;DMSwarm_X0&#34;,
            self,
            self.cdim,
            dtype=float,
            _register=True,
            _proxy=False,
            rebuild_on_cycle=False,
        )

        # This is for swarm streak management:
        # add variable to hold swarm origins

        if self.recycle_rate &gt; 1:

            self._remeshed = uw.swarm.SwarmVariable(
                &#34;DMSwarm_remeshed&#34;,
                self,
                1,
                dtype=int,
                _register=True,
                _proxy=False,
                rebuild_on_cycle=False,
            )

        self._X0_uninitialised = True
        self._index = None
        self._nnmapdict = {}

        super().__init__()

    @property
    def mesh(self):
        return self._mesh

    @property
    def data(self):
        return self.particle_coordinates.data

    @property
    def particle_coordinates(self):
        return self._coord_var

    @timing.routine_timer_decorator
    def populate(
        self,
        fill_param: Optional[int] = 1,
    ):
        &#34;&#34;&#34;
        Populate the swarm with particles throughout the domain.

        Parameters
        ----------
        fill_param:
            Parameter determining the particle count per cell (per dimension)
            for the given layout, using the mesh degree.
        &#34;&#34;&#34;

        self.fill_param = fill_param

        newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
        newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

        valid = newp_cells0 != -1
        newp_coords = newp_coords0[valid]
        newp_cells = newp_cells0[valid]

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(newp_coords.shape[0] + 1)

        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        coords[...] = newp_coords[...]
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

        if self.recycle_rate &gt; 1:
            with self.access():
                # This is a mesh-local quantity, so let&#39;s just
                # store it on the mesh in an ad_hoc fashion for now

                self.mesh.particle_X_orig = self.particle_coordinates.data.copy()

            with self.access():
                swarm_orig_size = self.particle_coordinates.data.shape[0]
                all_local_coords = np.vstack(
                    (self.particle_coordinates.data,) * (self.recycle_rate)
                )

                swarm_new_size = all_local_coords.data.shape[0]

            self.dm.addNPoints(swarm_new_size - swarm_orig_size)

            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

            coords[...] = (
                all_local_coords[...]
                + (0.33 / (1 + fill_param))
                * (np.random.random(size=all_local_coords.shape) - 0.5)
                * 0.00001
                * self.mesh._search_lengths[all_local_cells]  # typical cell size
            )

            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

            ## Now set the cycle values

            with self.access(self._remeshed):
                for i in range(0, self.recycle_rate):
                    offset = swarm_orig_size * i
                    self._remeshed.data[offset::, 0] = i

        return

    @timing.routine_timer_decorator
    def migrate(
        self,
        remove_sent_points=True,
        delete_lost_points=True,
        max_its=10,
    ):
        &#34;&#34;&#34;
        Migrate swarm across processes after coordinates have been updated.

        The algorithm uses a global kD-tree for the centroids of the domains to decide the particle mpi.rank (send to the closest)
        If the particles are mis-assigned to a particular mpi.rank, the next choice is the second-closest and so on.

        A few particles are still not found after this distribution process which probably means they are just outside the mesh.
        If some points remain lost, they will be deleted if `delete_lost_points` is set.

        Implementation note:
            We retained (above) the name `DMSwarmPIC_coor` for the particle field to allow this routine to be inherited by a PIC swarm
            which has this field pre-defined. (We&#39;d need to add a cellid field as well, and re-compute it upon landing)
        &#34;&#34;&#34;

        from time import time

        time_c = time()
        centroids = self.mesh._get_domain_centroids()
        mesh_domain_kdtree = uw.kdtree.KDTree(centroids)

        time0 = time()
        time1 = time()

        # This will only worry about particles that are not already claimed !
        #

        swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        in_or_not = self.mesh.points_in_domain(swarm_coord_array)
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

        num_points_in_domain = np.count_nonzero(in_or_not == True)
        num_points_not_in_domain = np.count_nonzero(in_or_not == False)
        not_my_points = np.where(in_or_not == False)[0]

        uw.mpi.barrier()

        global_unclaimed_points = int(
            uw.utilities.gather_data(
                num_points_not_in_domain, bcast=True, dtype=int
            ).sum()
        )

        global_claimed_points = int(
            uw.utilities.gather_data(num_points_in_domain, bcast=True, dtype=int).sum()
        )

        # Unlikely, but we should check this
        uw.mpi.barrier()
        if global_unclaimed_points == 0:
            return

        # Migrate particles between processors if appropriate
        # Otherwise skip the next step and just remove missing points
        # and tidy up.

        if uw.mpi.size &gt; 1:
            for it in range(0, min(max_its, uw.mpi.size)):

                # Send unclaimed points to next processor in line

                swarm_rank_array = self.dm.getField(&#34;DMSwarm_rank&#34;)
                swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                    (-1, self.dim)
                )

                if len(swarm_coord_array &gt; 0):
                    dist, rank = mesh_domain_kdtree.query(
                        swarm_coord_array[not_my_points],
                        k=it + 1,
                    )

                    swarm_rank_array[not_my_points, 0] = rank.reshape(-1, it + 1)[:, it]

                self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
                self.dm.restoreField(&#34;DMSwarm_rank&#34;)

                # Now we send the points (basic migration)
                self.dm.migrate(remove_sent_points=True)
                uw.mpi.barrier()

                swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                    (-1, self.dim)
                )

                in_or_not = self.mesh.points_in_domain(swarm_coord_array)
                self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

                num_points_in_domain = np.count_nonzero(in_or_not == True)
                num_points_not_in_domain = np.count_nonzero(in_or_not == False)
                not_my_points = np.where(in_or_not == False)[0]

                unclaimed_points_last_iteration = global_unclaimed_points
                claimed_points_last_iteration = global_claimed_points

                global_unclaimed_points = int(
                    uw.utilities.gather_data(
                        num_points_not_in_domain,
                        bcast=True,
                        dtype=int,
                    ).sum()
                )

                global_claimed_points = int(
                    uw.utilities.gather_data(
                        num_points_in_domain, bcast=True, dtype=int
                    ).sum()
                )

                if (
                    global_unclaimed_points == unclaimed_points_last_iteration
                    and global_claimed_points == claimed_points_last_iteration
                ):
                    break

        # Missing points for deletion if required
        if delete_lost_points:

            # print(
            #     f&#34;{uw.mpi.rank} - Delete {len(not_my_points)} from swarm size {self.dm.getLocalSize()}&#34;,
            #     flush=True,
            # )

            uw.mpi.barrier()
            if len(not_my_points &gt; 0):
                indices = np.sort(not_my_points)[::-1]
                for index in indices:
                    self.dm.removePointAtIndex(index)

            # print(
            #     f&#34;{uw.mpi.rank} - final swarm size {self.dm.getLocalSize()}&#34;,
            #     flush=True,
            # )

        return

    @timing.routine_timer_decorator
    def add_particles_with_coordinates(self, coordinatesArray) -&gt; int:
        &#34;&#34;&#34;
        Add particles to the swarm using particle coordinates provided
        using a numpy array.

        Note that particles with coordinates NOT local to the current processor will
        be rejected / ignored.

        Either include an array with all coordinates to all processors
        or an array with the local coordinates.

        Parameters
        ----------
        coordinatesArray : numpy.ndarray
            The numpy array containing the coordinate of the new particles. Array is
            expected to take shape n*dim, where n is the number of new particles, and
            dim is the dimensionality of the swarm&#39;s supporting mesh.

        Returns
        --------
        npoints: int
            The number of points added to the local section of the swarm.
        &#34;&#34;&#34;

        if not isinstance(coordinatesArray, np.ndarray):
            raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
        if not len(coordinatesArray.shape) == 2:
            raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
        if not coordinatesArray.shape[1] == self.mesh.dim:
            #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
            raise ValueError(
                &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                              number of particles to add, and &#39;dim&#39; is the dimensionality of
                              the supporting mesh ({}).&#34;&#34;&#34;.format(
                    self.mesh.dim
                )
            )

        valid = self.mesh.points_in_domain(coordinatesArray, strict_validation=True)
        valid_coordinates = coordinatesArray[valid]
        npoints = len(valid_coordinates)
        swarm_size = self.dm.getLocalSize()

        # -1 means no particles have been added yet (PETSc interface change)
        if swarm_size == -1:
            swarm_size = 0
            npoints = npoints + 1

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(npoints=npoints)

        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        coords[swarm_size::, :] = valid_coordinates[:, :]
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

        # Here we update the swarm cycle values as required

        if self.recycle_rate &gt; 1:
            with self.access(self._remeshed):
                # self._Xorig.data[...] = coordinatesArray
                self._remeshed.data[...] = 0

        self.dm.migrate(remove_sent_points=True)
        return npoints

    @timing.routine_timer_decorator
    def add_particles_with_global_coordinates(
        self, globalCoordinatesArray, migrate=True
    ) -&gt; int:
        &#34;&#34;&#34;
        Add particles to the swarm using particle coordinates provided
        using a numpy array.

        global coordinates: particles will be appropriately migrated

        Parameters
        ----------
        globalCoordinatesArray : numpy.ndarray
            The numpy array containing the coordinate of the new particles. Array is
            expected to take shape n*dim, where n is the number of new particles, and
            dim is the dimensionality of the swarm&#39;s supporting mesh.

        Returns
        --------
        npoints: int
            The number of points added to the local section of the swarm.
        &#34;&#34;&#34;

        if not isinstance(globalCoordinatesArray, np.ndarray):
            raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
        if not len(globalCoordinatesArray.shape) == 2:
            raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
        if not globalCoordinatesArray.shape[1] == self.mesh.dim:
            #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
            raise ValueError(
                &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                                number of particles to add, and &#39;dim&#39; is the dimensionality of
                                the supporting mesh ({}).&#34;&#34;&#34;.format(
                    self.mesh.dim
                )
            )

        npoints = len(globalCoordinatesArray)
        swarm_size = self.dm.getLocalSize()

        # -1 means no particles have been added yet
        if swarm_size == -1:
            swarm_size = 0
            npoints = npoints + 1

        self.dm.finalizeFieldRegister()
        self.dm.addNPoints(npoints=npoints)

        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        coords[swarm_size::, :] = globalCoordinatesArray[:, :]
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

        # Here we update the swarm cycle values as required

        if self.recycle_rate &gt; 1:
            with self.access(self._remeshed):
                # self._Xorig.data[...] = globalCoordinatesArray
                self._remeshed.data[...] = 0

        if migrate:
            self.migrate(remove_sent_points=True)

        return npoints

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm coordinates to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm checkpoint file to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

        if h5py.h5.get_config().mpi == True and not force_sequential:
            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores. This is probably why the parallel
            # h5py write hangs

            with self.access():
                data_copy = self.data[:].copy()

            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy[:],
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])

            del data_copy

        else:
            # It seems to be a bad idea to mix mpi barriers with the access
            # context manager so the copy-free version of this seems to hang
            # when there are many active cores

            with self.access():
                data_copy = self.data[:].copy()

            if comm.rank == 0:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;coordinates&#34;,
                            data=data_copy,
                            chunks=True,
                            maxshape=(None, data_copy.shape[1]),
                        )

            comm.barrier()
            for i in range(1, comm.size):
                if comm.rank == i:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;coordinates&#34;].resize(
                            (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                            axis=0,
                        )
                        # passive swarm, zero local particles is not unusual
                        if data_copy.shape[0] &gt; 0:
                            h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
                comm.barrier()
            comm.barrier()

            del data_copy

        return

    @timing.routine_timer_decorator
    def read_timestep(
        self,
        base_filename: str,
        swarm_id: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):
        output_base_name = os.path.join(outputPath, base_filename)
        swarm_file = output_base_name + f&#34;.{swarm_id}.{index:05}.h5&#34;

        ### open up file with coords on all procs
        with h5py.File(f&#34;{swarm_file}&#34;, &#34;r&#34;) as h5f:
            coordinates = h5f[&#34;coordinates&#34;][:]

        #### utilises the UW function for adding a swarm by an array
        self.add_particles_with_coordinates(coordinates)

        return

    @timing.routine_timer_decorator
    def add_variable(
        self,
        name,
        size=1,
        dtype=float,
        proxy_degree=2,
        _nn_proxy=False,
    ):
        return SwarmVariable(
            name,
            self,
            size,
            dtype=dtype,
            proxy_degree=proxy_degree,
            _nn_proxy=_nn_proxy,
        )

    @timing.routine_timer_decorator
    def petsc_save_checkpoint(
        self,
        swarmName: str,
        index: int,
        outputPath: Optional[str] = &#34;&#34;,
    ):
        &#34;&#34;&#34;

        Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        &#34;&#34;&#34;

        x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:05d}.xmf&#34;
        self.dm.viewXDMF(x_swarm_fname)

    @timing.routine_timer_decorator
    def write_timestep(
        self,
        filename: str,
        swarmname: str,
        index: int,
        swarmVars: Optional[list] = None,
        outputPath: Optional[str] = &#34;&#34;,
        time: Optional[int] = None,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential: Optional[bool] = False,
    ):
        &#34;&#34;&#34;

        Save data to h5 and a corresponding xdmf for visualisation using h5py.

        Parameters
        ----------
        swarmName :
            Name of the swarm to save.
        swarmVars :
            List of swarm objects to save.
        index :
            An index which might correspond to the timestep or output number (for example).
        outputPath :
            Path to save the data. If left empty it will save the data in the current working directory.
        time :
            Attach the time to the generated xdmf.
        compression :
            Whether to compress the h5 files [bool].
        compressionType :
            The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
        &#34;&#34;&#34;

        # This will eliminate the issue of whether or not to put path separators in the
        # outputPath. Also does the right thing if outputPath is &#34;&#34;

        output_base_name = os.path.join(outputPath, filename) + &#34;.&#34; + swarmname

        # check the directory where we will write checkpoint
        dir_path = os.path.dirname(output_base_name)  # get directory

        # check if path exists
        if os.path.exists(os.path.abspath(dir_path)):  # easier to debug abs
            pass
        else:
            raise RuntimeError(f&#34;{os.path.abspath(dir_path)} does not exist&#34;)

        # check if we have write access
        if os.access(os.path.abspath(dir_path), os.W_OK):
            pass
        else:
            raise RuntimeError(f&#34;No write access to {os.path.abspath(dir_path)}&#34;)

        # could also try to coerce this to be a list and raise if it fails (tuple, singleton ... )
        # also ... why the typechecking if this can still happen

        if swarmVars is not None and not isinstance(swarmVars, list):
            raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

        else:
            ### save the swarm particle location
            self.save(
                filename=f&#34;{output_base_name}.{index:05d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

        #### Generate a h5 file for each field
        if swarmVars != None:
            for field in swarmVars:
                field.save(
                    filename=f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;,
                    compression=compression,
                    compressionType=compressionType,
                    force_sequential=force_sequential,
                )

        if uw.mpi.rank == 0:
            ### only need to combine the h5 files to a single xdmf on one proc
            with open(f&#34;{output_base_name}.{index:05d}.xdmf&#34;, &#34;w&#34;) as xdmf:
                # Write the XDMF header
                xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
                xdmf.write(
                    &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
                )
                xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
                xdmf.write(
                    f&#39;&lt;Grid Name=&#34;{output_base_name}.{index:05d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
                )

                if time != None:
                    xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

                # Write the grid element for the HDF5 dataset
                with h5py.File(f&#34;{output_base_name}.{index:05}.h5&#34;, &#34;r&#34;) as h5f:
                    xdmf.write(
                        f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                    )
                    if h5f[&#34;coordinates&#34;].shape[1] == 2:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                    elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                        xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                    xdmf.write(
                        f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                    )
                    xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

                # Write the attribute element for the field
                if swarmVars != None:
                    for field in swarmVars:
                        with h5py.File(
                            f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;, &#34;r&#34;
                        ) as h5f:
                            if h5f[&#34;data&#34;].dtype == np.int32:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 1:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )
                            else:
                                xdmf.write(
                                    f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                                )
                                xdmf.write(
                                    f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                                )

                            xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
                else:
                    pass

                # Write the XDMF footer
                xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
                xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)

    @property
    def vars(self):
        return self._vars

    def access(self, *writeable_vars: SwarmVariable):
        &#34;&#34;&#34;
        This context manager makes the underlying swarm variables data available to
        the user. The data should be accessed via the variables `data` handle.

        As default, all data is read-only. To enable writeable data, the user should
        specify which variable they wish to modify.

        At the conclusion of the users context managed block, numerous further operations
        will be automatically executed. This includes swarm parallel migration routines
        where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
        variable proxy mesh variables will also be updated for modifed swarm variables.

        Parameters
        ----------
        writeable_vars
            The variables for which data write access is required.

        Example
        -------

        &gt;&gt;&gt; import underworld3 as uw
        &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
        &gt;&gt;&gt; with someMesh.deform_mesh():
        ...     someMesh.data[0] = [0.1,0.1]
        &gt;&gt;&gt; someMesh.data[0]
        array([ 0.1,  0.1])
        &#34;&#34;&#34;
        import time

        uw.timing._incrementDepth()
        stime = time.time()

        deaccess_list = []
        for var in self._vars.values():
            # if already accessed within higher level context manager, continue.
            if var._is_accessed == True:
                continue
            # set flag so variable status can be known elsewhere
            var._is_accessed = True
            # add to de-access list to rewind this later
            deaccess_list.append(var)
            # grab numpy object, setting read only if necessary
            var._data = self.dm.getField(var.clean_name).reshape(
                (-1, var.num_components)
            )
            assert var._data is not None
            if var not in writeable_vars:
                var._old_data_flag = var._data.flags.writeable
                var._data.flags.writeable = False
            else:
                # increment variable state
                var._increment()

            # make view for each var component
            if var._proxy:
                for i in range(0, var.shape[0]):
                    for j in range(0, var.shape[1]):
                        var._data_container[i, j] = var._data_container[i, j]._replace(
                            data=var.data[:, var._data_layout(i, j)],
                        )

        # if particles moving, update swarm state
        if self.particle_coordinates in writeable_vars:
            self._increment()

        # Create a class which specifies the required context
        # manager hooks (`__enter__`, `__exit__`).
        class exit_manager:
            def __init__(self, swarm):
                self.em_swarm = swarm

            def __enter__(self):

                pass

            def __exit__(self, *args):

                for var in self.em_swarm.vars.values():
                    # only de-access variables we have set access for.
                    if var not in deaccess_list:
                        continue
                    # set this back, although possibly not required.
                    if var not in writeable_vars:
                        var._data.flags.writeable = var._old_data_flag
                    var._data = None
                    self.em_swarm.dm.restoreField(var.clean_name)
                    var._is_accessed = False
                # do particle migration if coords changes

                if self.em_swarm.particle_coordinates in writeable_vars:
                    # let&#39;s use the mesh index to update the particles owning cells.
                    # note that the `petsc4py` interface is more convenient here as the
                    # `SwarmVariable.data` interface is controlled by the context manager
                    # that we are currently within, and it is therefore too easy to
                    # get things wrong that way.
                    #

                    if uw.mpi.size &gt; 1:

                        coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                            (-1, self.em_swarm.dim)
                        )

                        self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

                        ## We&#39;ll need to identify the new processes here and update the particle rank value accordingly
                        self.em_swarm.migrate(remove_sent_points=True)

                    # void these things too
                    self.em_swarm._index = None
                    self.em_swarm._nnmapdict = {}

                # do var updates
                for var in self.em_swarm.vars.values():
                    # if swarm migrated, update all.
                    # if var updated, update var.
                    if (self.em_swarm.particle_coordinates in writeable_vars) or (
                        var in writeable_vars
                    ):
                        var._update()

                    if var._proxy:
                        for i in range(0, var.shape[0]):
                            for j in range(0, var.shape[1]):
                                # var._data_ij[i, j] = None
                                var._data_container[i, j] = var._data_container[
                                    i, j
                                ]._replace(
                                    data=f&#34;SwarmVariable[...].data is only available within mesh.access() context&#34;,
                                )

                uw.timing._decrementDepth()
                uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

        return exit_manager(self)

    ## Better to have one master copy - this one is cut&#39;n&#39;pasted from
    ## the MeshVariable class

    def _data_layout(self, i, j=None):
        # mapping

        if self.vtype == uw.VarType.SCALAR:
            return 0
        if self.vtype == uw.VarType.VECTOR:
            if j is None:
                return i
            elif i == 0:
                return j
            else:
                raise IndexError(
                    f&#34;Vectors have shape {self.mesh.dim} or {(1, self.mesh.dim)} &#34;
                )
        if self.vtype == uw.VarType.TENSOR:
            if self.mesh.dim == 2:
                return ((0, 1), (2, 3))[i][j]
            else:
                return ((0, 1, 2), (3, 4, 5), (6, 7, 8))[i][j]

        if self.vtype == uw.VarType.SYM_TENSOR:
            if self.mesh.dim == 2:
                return ((0, 2), (2, 1))[i][j]
            else:
                return ((0, 3, 4), (3, 1, 5), (4, 5, 2))[i][j]

        if self.vtype == uw.VarType.MATRIX:
            return i + j * self.shape[0]

    ## Check this - the interface to kdtree has changed, are we picking the correct field ?
    @timing.routine_timer_decorator
    def _get_map(self, var):
        # generate tree if not avaiable
        if not self._index:
            with self.access():
                self._index = uw.kdtree.KDTree(self.data)

        # get or generate map
        meshvar_coords = var._meshVar.coords
        # we can&#39;t use numpy arrays directly as keys in python dicts, so
        # we&#39;ll use `xxhash` to generate a hash of array.
        # this shouldn&#39;t be an issue performance wise but we should test to be
        # sufficiently confident of this.
        import xxhash

        h = xxhash.xxh64()
        h.update(meshvar_coords)
        digest = h.intdigest()
        if digest not in self._nnmapdict:
            self._nnmapdict[digest] = self._index.query(meshvar_coords, k=1)[1]
        return self._nnmapdict[digest]

    @timing.routine_timer_decorator
    def advection(
        self,
        V_fn,
        delta_t,
        order=2,
        corrector=False,
        restore_points_to_domain_func=None,
        evalf=False,
        step_limit=False,
    ):

        dt_limit = self.estimate_dt(V_fn)

        if step_limit and dt_limit is not None:
            substeps = int(max(1, round(abs(delta_t) / dt_limit)))
        else:
            substeps = 1

        if uw.mpi.rank == 0 and self.verbose:
            print(f&#34;Substepping {substeps} / {abs(delta_t) / dt_limit}, {delta_t} &#34;)

        # X0 holds the particle location at the start of advection
        # This is needed because the particles may be migrated off-proc
        # during timestepping.

        X0 = self._X0

        V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

        # Use current velocity to estimate where the particles would have
        # landed in an implicit step. WE CANT DO THIS WITH SUB-STEPPING unless
        # We have a lot more information about the previous launch point / timestep
        # Also: how does this interact with the particle restoration function ?

        # if corrector == True and not self._X0_uninitialised:
        #     with self.access(self.particle_coordinates):
        #         v_at_Vpts = np.zeros_like(self.data)

        #         if evalf:
        #             for d in range(self.dim):
        #                 v_at_Vpts[:, d] = uw.function.evalf(
        #                     V_fn_matrix[d], self.data
        #                 ).reshape(-1)
        #         else:
        #             for d in range(self.dim):
        #                 v_at_Vpts[:, d] = uw.function.evaluate(
        #                     V_fn_matrix[d], self.data
        #                 ).reshape(-1)

        #         corrected_position = X0.data.copy() + delta_t * v_at_Vpts
        #         if restore_points_to_domain_func is not None:
        #             corrected_position = restore_points_to_domain_func(
        #                 corrected_position
        #             )

        #         updated_current_coords = 0.5 * (corrected_position + self.data.copy())

        #         # validate_coords to ensure they live within the domain (or there will be trouble)

        #         if restore_points_to_domain_func is not None:
        #             updated_current_coords = restore_points_to_domain_func(
        #                 updated_current_coords
        #             )

        #         self.data[...] = updated_current_coords[...]

        #         del updated_current_coords
        #         del v_at_Vpts

        # Wrap this whole thing in sub-stepping loop
        for step in range(0, substeps):

            with self.access(X0):
                X0.data[...] = self.particle_coordinates.data[...]

            # Mid point algorithm (2nd order)

            if order == 2:
                with self.access(self.particle_coordinates):
                    v_at_Vpts = np.zeros_like(self.particle_coordinates.data)

                    ##
                    ## Here we should check for particles which are interpolated and
                    ## those which can only be extrapolated. For the former, evalf is
                    ## not needed but for the latter it is essential
                    ##

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.particle_coordinates.data
                    #         ).reshape(-1)
                    # else:
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.particle_coordinates.data,
                            evalf=evalf,
                        ).reshape(-1)

                    mid_pt_coords = (
                        self.particle_coordinates.data[...]
                        + 0.5 * delta_t * v_at_Vpts / substeps
                    )

                    if restore_points_to_domain_func is not None:
                        mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                    self.particle_coordinates.data[...] = mid_pt_coords[...]

                    del mid_pt_coords

                    ## Let the swarm be updated, and then move the rest of the way

                    v_at_Vpts = np.zeros_like(self.data)

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.particle_coordinates.data
                    #         ).reshape(-1)
                    # else:
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.particle_coordinates.data,
                            evalf=evalf,
                        ).reshape(-1)

                    # if (uw.mpi.rank == 0):
                    #     print(&#34;Re-launch from X0&#34;, flush=True)

                    new_coords = X0.data[...] + delta_t * v_at_Vpts / substeps

                    if restore_points_to_domain_func is not None:
                        new_coords = restore_points_to_domain_func(new_coords)

                    self.particle_coordinates.data[...] = new_coords[...]

                    del new_coords
                    del v_at_Vpts

            # forward Euler (1st order)
            else:

                with self.access(self.particle_coordinates):
                    v_at_Vpts = np.zeros_like(self.data)

                    # if evalf:
                    #     for d in range(self.dim):
                    #         v_at_Vpts[:, d] = uw.function.evalf(
                    #             V_fn_matrix[d], self.data
                    #         ).reshape(-1)
                    # else:
                    for d in range(self.dim):
                        v_at_Vpts[:, d] = uw.function.evaluate(
                            V_fn_matrix[d],
                            self.data,
                            evalf=evalf,
                        ).reshape(-1)

                    new_coords = self.data + delta_t * v_at_Vpts / substeps

                    if restore_points_to_domain_func is not None:
                        new_coords = restore_points_to_domain_func(new_coords)

                    self.data[...] = new_coords[...].copy()

        ## End of substepping loop

        ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
        ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
        ## original location.

        if self.recycle_rate &gt; 1:
            # Restore particles which have cycle == cycle rate (use &gt;= just in case)

            # Remove remesh points and recreate a new set at the mesh-local
            # locations that we already have stored.

            with self.access(self.particle_coordinates, self._remeshed):
                remeshed = self._remeshed.data[:, 0] == 0
                # This is one way to do it ... we can do this better though
                self.data[remeshed, 0] = 1.0e100

            swarm_size = self.dm.getLocalSize()

            num_remeshed_points = self.mesh.particle_X_orig.shape[0]

            self.dm.addNPoints(num_remeshed_points)

            ## cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
            coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
            rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

            # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
            # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
            # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

            perturbation = 0.00001 * (
                (0.33 / (1 + self.fill_param))
                * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
                * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
            )

            coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
            ## cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
            rmsh[swarm_size::] = 0

            # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

            # when we let this go, the particles may be re-distributed to
            # other processors, and we will need to rebuild the remeshed
            # array before trying to compute / assign values to variables

            for swarmVar in self.vars.values():
                if swarmVar._rebuild_on_cycle:
                    with self.access(swarmVar):
                        if swarmVar.dtype is int:
                            nnn = 1
                        else:
                            nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                        interpolated_values = (
                            swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                            #     swarmVar._meshVar.fn, self.mesh.particle_X_orig
                            # )
                        ).astype(swarmVar.dtype)

                        swarmVar.data[swarm_size::] = interpolated_values

            ##
            ## Determine RANK
            ##

            # Migrate will already have been called by the access manager.
            # Maybe we should hash the local particle coords to make this
            # a little more user-friendly

            # self.dm.migrate(remove_sent_points=True)

            with self.access(self._remeshed):
                self._remeshed.data[...] = np.mod(
                    self._remeshed.data[...] - 1, self.recycle_rate
                )

            self.cycle += 1

            ## End of cycle_swarm loop
            #

        # Remove points no longer in the domain
        self.migrate(
            delete_lost_points=True,
            max_its=1,
        )

        return

    @timing.routine_timer_decorator
    def estimate_dt(self, V_fn):
        &#34;&#34;&#34;
        Calculates an appropriate advective timestep for the given
        mesh and velocity configuration.
        &#34;&#34;&#34;
        # we&#39;ll want to do this on an element by element basis
        # for more general mesh

        # first let&#39;s extract a max global velocity magnitude
        import math

        with self.access():
            vel = uw.function.evaluate(V_fn, self.particle_coordinates.data, evalf=True)
            try:
                magvel_squared = vel[:, 0] ** 2 + vel[:, 1] ** 2
                if self.mesh.dim == 3:
                    magvel_squared += vel[:, 2] ** 2

                max_magvel = math.sqrt(magvel_squared.max())

            except (ValueError, IndexError):
                max_magvel = 0.0

        from mpi4py import MPI

        max_magvel_glob = comm.allreduce(max_magvel, op=MPI.MAX)

        min_dx = self.mesh.get_min_radius()

        # The assumption should be that we cross one or two elements (2-4 radii), not more,
        # in a single step (order 2, means one element per half-step or something
        # that we can broadly interpret that way)

        if max_magvel_glob != 0.0:
            return min_dx / max_magvel_glob
        else:
            return None</code></pre>
</details>
<div class="desc"><p>A basic particle swarm implementation for Lagrangian particle tracking and data storage.</p>
<p>The UW <code><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></code> class provides a simplified particle management system that uses
PETSc's DMSWARM_BASIC type. Unlike the standard <code><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></code> class, this implementation
does not rely on PETSc to determine ranks for particle migration but instead uses
our own kdtree neighbour-domain computations.</p>
<p>This class is preferred for most operations except where particle / cell relationships
are always required.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mesh</code></strong> :&ensp;<code>uw.discretisation.Mesh</code></dt>
<dd>The mesh object that defines the computational domain for particle operations.
Particles will be associated with this mesh for spatial queries and operations.</dd>
<dt><strong><code>recycle_rate</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Rate at which particles are recycled for streak management. If &gt; 1, enables
streak particle functionality where particles are duplicated and tracked
across multiple cycles. Default is 0 (no recycling).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Enable verbose output for debugging and monitoring particle operations.
Default is False.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>mesh</code></strong> :&ensp;<code>uw.discretisation.Mesh</code></dt>
<dd>Reference to the associated mesh object.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Spatial dimension of the mesh (2D or 3D).</dd>
<dt><strong><code>cdim</code></strong> :&ensp;<code>int</code></dt>
<dd>Coordinate dimension of the mesh.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Direct access to particle coordinate data.</dd>
<dt><strong><code>particle_coordinates</code></strong> :&ensp;<code><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></code></dt>
<dd>SwarmVariable containing particle coordinate information.</dd>
<dt><strong><code>recycle_rate</code></strong> :&ensp;<code>int</code></dt>
<dd>Current recycle rate for streak management.</dd>
<dt><strong><code>cycle</code></strong> :&ensp;<code>int</code></dt>
<dd>Current cycle number for streak particles.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>populate(fill_param=1)
Populate the swarm with particles throughout the domain.
migrate(remove_sent_points=True, delete_lost_points=True, max_its=10)
Manually migrate particles across MPI processes after coordinate updates.
add_particles_with_coordinates(coords)
Add new particles at specified coordinate locations.
add_particles_with_global_coordinates(coords)
Add particles using global coordinate system.
add_variable(name, size, dtype=float)
Add a new variable to track additional particle properties.
save(filename, meshUnits=1.0, swarmUnits=1.0, units="dimensionless")
Save swarm data to file.
read_timestep(filename, step_name, outputPath="./output/")
Read swarm data from a specific timestep file.
advection(V_fn, delta_t, evalf=False, corrector=True, restore_points_func=None)
Advect particles using a velocity field.
estimate_dt(V_fn, dt_min=1.0e-15, dt_max=1.0)
Estimate appropriate timestep for particle advection.</p>
<h2 id="examples">Examples</h2>
<p>Create a basic swarm and populate with particles:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import underworld3 as uw
&gt;&gt;&gt; mesh = uw.meshing.UnstructuredSimplexBox(minCoords=(0,0), maxCoords=(1,1))
&gt;&gt;&gt; swarm = uw.swarm.Swarm(mesh=mesh)
&gt;&gt;&gt; swarm.populate(fill_param=2)
</code></pre>
<p>Create a streak swarm with recycling:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; streak_swarm = uw.swarm.Swarm(mesh=mesh, recycle_rate=5)
&gt;&gt;&gt; streak_swarm.populate(fill_param=1)
</code></pre>
<p>Add custom particle data:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; temperature = swarm.add_variable(&quot;temperature&quot;, 1)
&gt;&gt;&gt; velocity = swarm.add_variable(&quot;velocity&quot;, mesh.dim)
</code></pre>
<p>Manual particle migration after coordinate updates:</p>
<p>Note: particle migration is still called automatically when we
<code>access</code> and update the particle_coordinates variables</p>
<p>Note: <code>swarm.populate</code> uses a the mesh point locations for discontinuous interpolants to
determine the particle locations.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.NodalPointUWSwarm" href="#underworld3.swarm.NodalPointUWSwarm">NodalPointUWSwarm</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.Swarm.instances"><code class="name">var <span class="ident">instances</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.Swarm.data"><code class="name">prop <span class="ident">data</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self):
    return self.particle_coordinates.data</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.mesh"><code class="name">prop <span class="ident">mesh</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mesh(self):
    return self._mesh</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.particle_coordinates"><code class="name">prop <span class="ident">particle_coordinates</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def particle_coordinates(self):
    return self._coord_var</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.vars"><code class="name">prop <span class="ident">vars</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def vars(self):
    return self._vars</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.Swarm.access"><code class="name flex">
<span>def <span class="ident">access</span></span>(<span>self,<br>*writeable_vars: <a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def access(self, *writeable_vars: SwarmVariable):
    &#34;&#34;&#34;
    This context manager makes the underlying swarm variables data available to
    the user. The data should be accessed via the variables `data` handle.

    As default, all data is read-only. To enable writeable data, the user should
    specify which variable they wish to modify.

    At the conclusion of the users context managed block, numerous further operations
    will be automatically executed. This includes swarm parallel migration routines
    where the swarm&#39;s `particle_coordinates` variable has been modified. The swarm
    variable proxy mesh variables will also be updated for modifed swarm variables.

    Parameters
    ----------
    writeable_vars
        The variables for which data write access is required.

    Example
    -------

    &gt;&gt;&gt; import underworld3 as uw
    &gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
    &gt;&gt;&gt; with someMesh.deform_mesh():
    ...     someMesh.data[0] = [0.1,0.1]
    &gt;&gt;&gt; someMesh.data[0]
    array([ 0.1,  0.1])
    &#34;&#34;&#34;
    import time

    uw.timing._incrementDepth()
    stime = time.time()

    deaccess_list = []
    for var in self._vars.values():
        # if already accessed within higher level context manager, continue.
        if var._is_accessed == True:
            continue
        # set flag so variable status can be known elsewhere
        var._is_accessed = True
        # add to de-access list to rewind this later
        deaccess_list.append(var)
        # grab numpy object, setting read only if necessary
        var._data = self.dm.getField(var.clean_name).reshape(
            (-1, var.num_components)
        )
        assert var._data is not None
        if var not in writeable_vars:
            var._old_data_flag = var._data.flags.writeable
            var._data.flags.writeable = False
        else:
            # increment variable state
            var._increment()

        # make view for each var component
        if var._proxy:
            for i in range(0, var.shape[0]):
                for j in range(0, var.shape[1]):
                    var._data_container[i, j] = var._data_container[i, j]._replace(
                        data=var.data[:, var._data_layout(i, j)],
                    )

    # if particles moving, update swarm state
    if self.particle_coordinates in writeable_vars:
        self._increment()

    # Create a class which specifies the required context
    # manager hooks (`__enter__`, `__exit__`).
    class exit_manager:
        def __init__(self, swarm):
            self.em_swarm = swarm

        def __enter__(self):

            pass

        def __exit__(self, *args):

            for var in self.em_swarm.vars.values():
                # only de-access variables we have set access for.
                if var not in deaccess_list:
                    continue
                # set this back, although possibly not required.
                if var not in writeable_vars:
                    var._data.flags.writeable = var._old_data_flag
                var._data = None
                self.em_swarm.dm.restoreField(var.clean_name)
                var._is_accessed = False
            # do particle migration if coords changes

            if self.em_swarm.particle_coordinates in writeable_vars:
                # let&#39;s use the mesh index to update the particles owning cells.
                # note that the `petsc4py` interface is more convenient here as the
                # `SwarmVariable.data` interface is controlled by the context manager
                # that we are currently within, and it is therefore too easy to
                # get things wrong that way.
                #

                if uw.mpi.size &gt; 1:

                    coords = self.em_swarm.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                        (-1, self.em_swarm.dim)
                    )

                    self.em_swarm.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

                    ## We&#39;ll need to identify the new processes here and update the particle rank value accordingly
                    self.em_swarm.migrate(remove_sent_points=True)

                # void these things too
                self.em_swarm._index = None
                self.em_swarm._nnmapdict = {}

            # do var updates
            for var in self.em_swarm.vars.values():
                # if swarm migrated, update all.
                # if var updated, update var.
                if (self.em_swarm.particle_coordinates in writeable_vars) or (
                    var in writeable_vars
                ):
                    var._update()

                if var._proxy:
                    for i in range(0, var.shape[0]):
                        for j in range(0, var.shape[1]):
                            # var._data_ij[i, j] = None
                            var._data_container[i, j] = var._data_container[
                                i, j
                            ]._replace(
                                data=f&#34;SwarmVariable[...].data is only available within mesh.access() context&#34;,
                            )

            uw.timing._decrementDepth()
            uw.timing.log_result(time.time() - stime, &#34;Swarm.access&#34;, 1)

    return exit_manager(self)</code></pre>
</details>
<div class="desc"><p>This context manager makes the underlying swarm variables data available to
the user. The data should be accessed via the variables <code>data</code> handle.</p>
<p>As default, all data is read-only. To enable writeable data, the user should
specify which variable they wish to modify.</p>
<p>At the conclusion of the users context managed block, numerous further operations
will be automatically executed. This includes swarm parallel migration routines
where the swarm's <code>particle_coordinates</code> variable has been modified. The swarm
variable proxy mesh variables will also be updated for modifed swarm variables.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>writeable_vars</code></strong></dt>
<dd>The variables for which data write access is required.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import underworld3 as uw
&gt;&gt;&gt; someMesh = uw.discretisation.FeMesh_Cartesian()
&gt;&gt;&gt; with someMesh.deform_mesh():
...     someMesh.data[0] = [0.1,0.1]
&gt;&gt;&gt; someMesh.data[0]
array([ 0.1,  0.1])
</code></pre></div>
</dd>
<dt id="underworld3.swarm.Swarm.add_particles_with_coordinates"><code class="name flex">
<span>def <span class="ident">add_particles_with_coordinates</span></span>(<span>self, coordinatesArray) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_particles_with_coordinates(self, coordinatesArray) -&gt; int:
    &#34;&#34;&#34;
    Add particles to the swarm using particle coordinates provided
    using a numpy array.

    Note that particles with coordinates NOT local to the current processor will
    be rejected / ignored.

    Either include an array with all coordinates to all processors
    or an array with the local coordinates.

    Parameters
    ----------
    coordinatesArray : numpy.ndarray
        The numpy array containing the coordinate of the new particles. Array is
        expected to take shape n*dim, where n is the number of new particles, and
        dim is the dimensionality of the swarm&#39;s supporting mesh.

    Returns
    --------
    npoints: int
        The number of points added to the local section of the swarm.
    &#34;&#34;&#34;

    if not isinstance(coordinatesArray, np.ndarray):
        raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
    if not len(coordinatesArray.shape) == 2:
        raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
    if not coordinatesArray.shape[1] == self.mesh.dim:
        #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
        raise ValueError(
            &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                          number of particles to add, and &#39;dim&#39; is the dimensionality of
                          the supporting mesh ({}).&#34;&#34;&#34;.format(
                self.mesh.dim
            )
        )

    valid = self.mesh.points_in_domain(coordinatesArray, strict_validation=True)
    valid_coordinates = coordinatesArray[valid]
    npoints = len(valid_coordinates)
    swarm_size = self.dm.getLocalSize()

    # -1 means no particles have been added yet (PETSc interface change)
    if swarm_size == -1:
        swarm_size = 0
        npoints = npoints + 1

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(npoints=npoints)

    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
    coords[swarm_size::, :] = valid_coordinates[:, :]
    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

    # Here we update the swarm cycle values as required

    if self.recycle_rate &gt; 1:
        with self.access(self._remeshed):
            # self._Xorig.data[...] = coordinatesArray
            self._remeshed.data[...] = 0

    self.dm.migrate(remove_sent_points=True)
    return npoints</code></pre>
</details>
<div class="desc"><p>Add particles to the swarm using particle coordinates provided
using a numpy array.</p>
<p>Note that particles with coordinates NOT local to the current processor will
be rejected / ignored.</p>
<p>Either include an array with all coordinates to all processors
or an array with the local coordinates.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>coordinatesArray</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The numpy array containing the coordinate of the new particles. Array is
expected to take shape n*dim, where n is the number of new particles, and
dim is the dimensionality of the swarm's supporting mesh.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>npoints</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of points added to the local section of the swarm.</dd>
</dl></div>
</dd>
<dt id="underworld3.swarm.Swarm.add_particles_with_global_coordinates"><code class="name flex">
<span>def <span class="ident">add_particles_with_global_coordinates</span></span>(<span>self, globalCoordinatesArray, migrate=True) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_particles_with_global_coordinates(
    self, globalCoordinatesArray, migrate=True
) -&gt; int:
    &#34;&#34;&#34;
    Add particles to the swarm using particle coordinates provided
    using a numpy array.

    global coordinates: particles will be appropriately migrated

    Parameters
    ----------
    globalCoordinatesArray : numpy.ndarray
        The numpy array containing the coordinate of the new particles. Array is
        expected to take shape n*dim, where n is the number of new particles, and
        dim is the dimensionality of the swarm&#39;s supporting mesh.

    Returns
    --------
    npoints: int
        The number of points added to the local section of the swarm.
    &#34;&#34;&#34;

    if not isinstance(globalCoordinatesArray, np.ndarray):
        raise TypeError(&#34;&#39;coordinateArray&#39; must be provided as a numpy array&#34;)
    if not len(globalCoordinatesArray.shape) == 2:
        raise ValueError(&#34;The &#39;coordinateArray&#39; is expected to be two dimensional.&#34;)
    if not globalCoordinatesArray.shape[1] == self.mesh.dim:
        #### petsc appears to ignore columns that are greater than the mesh dim, but still worth including
        raise ValueError(
            &#34;&#34;&#34;The &#39;coordinateArray&#39; must have shape n*dim, where &#39;n&#39; is the
                            number of particles to add, and &#39;dim&#39; is the dimensionality of
                            the supporting mesh ({}).&#34;&#34;&#34;.format(
                self.mesh.dim
            )
        )

    npoints = len(globalCoordinatesArray)
    swarm_size = self.dm.getLocalSize()

    # -1 means no particles have been added yet
    if swarm_size == -1:
        swarm_size = 0
        npoints = npoints + 1

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(npoints=npoints)

    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
    coords[swarm_size::, :] = globalCoordinatesArray[:, :]
    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

    # Here we update the swarm cycle values as required

    if self.recycle_rate &gt; 1:
        with self.access(self._remeshed):
            # self._Xorig.data[...] = globalCoordinatesArray
            self._remeshed.data[...] = 0

    if migrate:
        self.migrate(remove_sent_points=True)

    return npoints</code></pre>
</details>
<div class="desc"><p>Add particles to the swarm using particle coordinates provided
using a numpy array.</p>
<p>global coordinates: particles will be appropriately migrated</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>globalCoordinatesArray</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The numpy array containing the coordinate of the new particles. Array is
expected to take shape n*dim, where n is the number of new particles, and
dim is the dimensionality of the swarm's supporting mesh.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>npoints</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of points added to the local section of the swarm.</dd>
</dl></div>
</dd>
<dt id="underworld3.swarm.Swarm.add_variable"><code class="name flex">
<span>def <span class="ident">add_variable</span></span>(<span>self, name, size=1, dtype=builtins.float, proxy_degree=2)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def add_variable(
    self,
    name,
    size=1,
    dtype=float,
    proxy_degree=2,
    _nn_proxy=False,
):
    return SwarmVariable(
        name,
        self,
        size,
        dtype=dtype,
        proxy_degree=proxy_degree,
        _nn_proxy=_nn_proxy,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.advection"><code class="name flex">
<span>def <span class="ident">advection</span></span>(<span>self,<br>V_fn,<br>delta_t,<br>order=2,<br>corrector=False,<br>restore_points_to_domain_func=None,<br>evalf=False,<br>step_limit=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def advection(
    self,
    V_fn,
    delta_t,
    order=2,
    corrector=False,
    restore_points_to_domain_func=None,
    evalf=False,
    step_limit=False,
):

    dt_limit = self.estimate_dt(V_fn)

    if step_limit and dt_limit is not None:
        substeps = int(max(1, round(abs(delta_t) / dt_limit)))
    else:
        substeps = 1

    if uw.mpi.rank == 0 and self.verbose:
        print(f&#34;Substepping {substeps} / {abs(delta_t) / dt_limit}, {delta_t} &#34;)

    # X0 holds the particle location at the start of advection
    # This is needed because the particles may be migrated off-proc
    # during timestepping.

    X0 = self._X0

    V_fn_matrix = self.mesh.vector.to_matrix(V_fn)

    # Use current velocity to estimate where the particles would have
    # landed in an implicit step. WE CANT DO THIS WITH SUB-STEPPING unless
    # We have a lot more information about the previous launch point / timestep
    # Also: how does this interact with the particle restoration function ?

    # if corrector == True and not self._X0_uninitialised:
    #     with self.access(self.particle_coordinates):
    #         v_at_Vpts = np.zeros_like(self.data)

    #         if evalf:
    #             for d in range(self.dim):
    #                 v_at_Vpts[:, d] = uw.function.evalf(
    #                     V_fn_matrix[d], self.data
    #                 ).reshape(-1)
    #         else:
    #             for d in range(self.dim):
    #                 v_at_Vpts[:, d] = uw.function.evaluate(
    #                     V_fn_matrix[d], self.data
    #                 ).reshape(-1)

    #         corrected_position = X0.data.copy() + delta_t * v_at_Vpts
    #         if restore_points_to_domain_func is not None:
    #             corrected_position = restore_points_to_domain_func(
    #                 corrected_position
    #             )

    #         updated_current_coords = 0.5 * (corrected_position + self.data.copy())

    #         # validate_coords to ensure they live within the domain (or there will be trouble)

    #         if restore_points_to_domain_func is not None:
    #             updated_current_coords = restore_points_to_domain_func(
    #                 updated_current_coords
    #             )

    #         self.data[...] = updated_current_coords[...]

    #         del updated_current_coords
    #         del v_at_Vpts

    # Wrap this whole thing in sub-stepping loop
    for step in range(0, substeps):

        with self.access(X0):
            X0.data[...] = self.particle_coordinates.data[...]

        # Mid point algorithm (2nd order)

        if order == 2:
            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.particle_coordinates.data)

                ##
                ## Here we should check for particles which are interpolated and
                ## those which can only be extrapolated. For the former, evalf is
                ## not needed but for the latter it is essential
                ##

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.particle_coordinates.data
                #         ).reshape(-1)
                # else:
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.particle_coordinates.data,
                        evalf=evalf,
                    ).reshape(-1)

                mid_pt_coords = (
                    self.particle_coordinates.data[...]
                    + 0.5 * delta_t * v_at_Vpts / substeps
                )

                if restore_points_to_domain_func is not None:
                    mid_pt_coords = restore_points_to_domain_func(mid_pt_coords)

                self.particle_coordinates.data[...] = mid_pt_coords[...]

                del mid_pt_coords

                ## Let the swarm be updated, and then move the rest of the way

                v_at_Vpts = np.zeros_like(self.data)

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.particle_coordinates.data
                #         ).reshape(-1)
                # else:
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.particle_coordinates.data,
                        evalf=evalf,
                    ).reshape(-1)

                # if (uw.mpi.rank == 0):
                #     print(&#34;Re-launch from X0&#34;, flush=True)

                new_coords = X0.data[...] + delta_t * v_at_Vpts / substeps

                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.particle_coordinates.data[...] = new_coords[...]

                del new_coords
                del v_at_Vpts

        # forward Euler (1st order)
        else:

            with self.access(self.particle_coordinates):
                v_at_Vpts = np.zeros_like(self.data)

                # if evalf:
                #     for d in range(self.dim):
                #         v_at_Vpts[:, d] = uw.function.evalf(
                #             V_fn_matrix[d], self.data
                #         ).reshape(-1)
                # else:
                for d in range(self.dim):
                    v_at_Vpts[:, d] = uw.function.evaluate(
                        V_fn_matrix[d],
                        self.data,
                        evalf=evalf,
                    ).reshape(-1)

                new_coords = self.data + delta_t * v_at_Vpts / substeps

                if restore_points_to_domain_func is not None:
                    new_coords = restore_points_to_domain_func(new_coords)

                self.data[...] = new_coords[...].copy()

    ## End of substepping loop

    ## Cycling of the swarm is a cheap and cheerful version of population control for particles. It turns the
    ## swarm into a streak-swarm where particles are Lagrangian for a number of steps and then reset to their
    ## original location.

    if self.recycle_rate &gt; 1:
        # Restore particles which have cycle == cycle rate (use &gt;= just in case)

        # Remove remesh points and recreate a new set at the mesh-local
        # locations that we already have stored.

        with self.access(self.particle_coordinates, self._remeshed):
            remeshed = self._remeshed.data[:, 0] == 0
            # This is one way to do it ... we can do this better though
            self.data[remeshed, 0] = 1.0e100

        swarm_size = self.dm.getLocalSize()

        num_remeshed_points = self.mesh.particle_X_orig.shape[0]

        self.dm.addNPoints(num_remeshed_points)

        ## cellid = self.dm.getField(&#34;DMSwarm_cellid&#34;)
        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
        rmsh = self.dm.getField(&#34;DMSwarm_remeshed&#34;)

        # print(f&#34;cellid -&gt; {cellid.shape}&#34;)
        # print(f&#34;particle coords -&gt; {coords.shape}&#34;)
        # print(f&#34;remeshed points  -&gt; {num_remeshed_points}&#34;)

        perturbation = 0.00001 * (
            (0.33 / (1 + self.fill_param))
            * (np.random.random(size=(num_remeshed_points, self.dim)) - 0.5)
            * self.mesh._radii[cellid[swarm_size::]].reshape(-1, 1)
        )

        coords[swarm_size::] = self.mesh.particle_X_orig[:, :] + perturbation
        ## cellid[swarm_size::] = self.mesh.particle_CellID_orig[:, 0]
        rmsh[swarm_size::] = 0

        # self.dm.restoreField(&#34;DMSwarm_cellid&#34;)
        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
        self.dm.restoreField(&#34;DMSwarm_remeshed&#34;)

        # when we let this go, the particles may be re-distributed to
        # other processors, and we will need to rebuild the remeshed
        # array before trying to compute / assign values to variables

        for swarmVar in self.vars.values():
            if swarmVar._rebuild_on_cycle:
                with self.access(swarmVar):
                    if swarmVar.dtype is int:
                        nnn = 1
                    else:
                        nnn = self.mesh.dim + 1  # 3 for triangles, 4 for tets ...

                    interpolated_values = (
                        swarmVar.rbf_interpolate(self.mesh.particle_X_orig, nnn=nnn)
                        #     swarmVar._meshVar.fn, self.mesh.particle_X_orig
                        # )
                    ).astype(swarmVar.dtype)

                    swarmVar.data[swarm_size::] = interpolated_values

        ##
        ## Determine RANK
        ##

        # Migrate will already have been called by the access manager.
        # Maybe we should hash the local particle coords to make this
        # a little more user-friendly

        # self.dm.migrate(remove_sent_points=True)

        with self.access(self._remeshed):
            self._remeshed.data[...] = np.mod(
                self._remeshed.data[...] - 1, self.recycle_rate
            )

        self.cycle += 1

        ## End of cycle_swarm loop
        #

    # Remove points no longer in the domain
    self.migrate(
        delete_lost_points=True,
        max_its=1,
    )

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.estimate_dt"><code class="name flex">
<span>def <span class="ident">estimate_dt</span></span>(<span>self, V_fn)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def estimate_dt(self, V_fn):
    &#34;&#34;&#34;
    Calculates an appropriate advective timestep for the given
    mesh and velocity configuration.
    &#34;&#34;&#34;
    # we&#39;ll want to do this on an element by element basis
    # for more general mesh

    # first let&#39;s extract a max global velocity magnitude
    import math

    with self.access():
        vel = uw.function.evaluate(V_fn, self.particle_coordinates.data, evalf=True)
        try:
            magvel_squared = vel[:, 0] ** 2 + vel[:, 1] ** 2
            if self.mesh.dim == 3:
                magvel_squared += vel[:, 2] ** 2

            max_magvel = math.sqrt(magvel_squared.max())

        except (ValueError, IndexError):
            max_magvel = 0.0

    from mpi4py import MPI

    max_magvel_glob = comm.allreduce(max_magvel, op=MPI.MAX)

    min_dx = self.mesh.get_min_radius()

    # The assumption should be that we cross one or two elements (2-4 radii), not more,
    # in a single step (order 2, means one element per half-step or something
    # that we can broadly interpret that way)

    if max_magvel_glob != 0.0:
        return min_dx / max_magvel_glob
    else:
        return None</code></pre>
</details>
<div class="desc"><p>Calculates an appropriate advective timestep for the given
mesh and velocity configuration.</p></div>
</dd>
<dt id="underworld3.swarm.Swarm.migrate"><code class="name flex">
<span>def <span class="ident">migrate</span></span>(<span>self, remove_sent_points=True, delete_lost_points=True, max_its=10)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def migrate(
    self,
    remove_sent_points=True,
    delete_lost_points=True,
    max_its=10,
):
    &#34;&#34;&#34;
    Migrate swarm across processes after coordinates have been updated.

    The algorithm uses a global kD-tree for the centroids of the domains to decide the particle mpi.rank (send to the closest)
    If the particles are mis-assigned to a particular mpi.rank, the next choice is the second-closest and so on.

    A few particles are still not found after this distribution process which probably means they are just outside the mesh.
    If some points remain lost, they will be deleted if `delete_lost_points` is set.

    Implementation note:
        We retained (above) the name `DMSwarmPIC_coor` for the particle field to allow this routine to be inherited by a PIC swarm
        which has this field pre-defined. (We&#39;d need to add a cellid field as well, and re-compute it upon landing)
    &#34;&#34;&#34;

    from time import time

    time_c = time()
    centroids = self.mesh._get_domain_centroids()
    mesh_domain_kdtree = uw.kdtree.KDTree(centroids)

    time0 = time()
    time1 = time()

    # This will only worry about particles that are not already claimed !
    #

    swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
    in_or_not = self.mesh.points_in_domain(swarm_coord_array)
    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

    num_points_in_domain = np.count_nonzero(in_or_not == True)
    num_points_not_in_domain = np.count_nonzero(in_or_not == False)
    not_my_points = np.where(in_or_not == False)[0]

    uw.mpi.barrier()

    global_unclaimed_points = int(
        uw.utilities.gather_data(
            num_points_not_in_domain, bcast=True, dtype=int
        ).sum()
    )

    global_claimed_points = int(
        uw.utilities.gather_data(num_points_in_domain, bcast=True, dtype=int).sum()
    )

    # Unlikely, but we should check this
    uw.mpi.barrier()
    if global_unclaimed_points == 0:
        return

    # Migrate particles between processors if appropriate
    # Otherwise skip the next step and just remove missing points
    # and tidy up.

    if uw.mpi.size &gt; 1:
        for it in range(0, min(max_its, uw.mpi.size)):

            # Send unclaimed points to next processor in line

            swarm_rank_array = self.dm.getField(&#34;DMSwarm_rank&#34;)
            swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                (-1, self.dim)
            )

            if len(swarm_coord_array &gt; 0):
                dist, rank = mesh_domain_kdtree.query(
                    swarm_coord_array[not_my_points],
                    k=it + 1,
                )

                swarm_rank_array[not_my_points, 0] = rank.reshape(-1, it + 1)[:, it]

            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)
            self.dm.restoreField(&#34;DMSwarm_rank&#34;)

            # Now we send the points (basic migration)
            self.dm.migrate(remove_sent_points=True)
            uw.mpi.barrier()

            swarm_coord_array = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape(
                (-1, self.dim)
            )

            in_or_not = self.mesh.points_in_domain(swarm_coord_array)
            self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

            num_points_in_domain = np.count_nonzero(in_or_not == True)
            num_points_not_in_domain = np.count_nonzero(in_or_not == False)
            not_my_points = np.where(in_or_not == False)[0]

            unclaimed_points_last_iteration = global_unclaimed_points
            claimed_points_last_iteration = global_claimed_points

            global_unclaimed_points = int(
                uw.utilities.gather_data(
                    num_points_not_in_domain,
                    bcast=True,
                    dtype=int,
                ).sum()
            )

            global_claimed_points = int(
                uw.utilities.gather_data(
                    num_points_in_domain, bcast=True, dtype=int
                ).sum()
            )

            if (
                global_unclaimed_points == unclaimed_points_last_iteration
                and global_claimed_points == claimed_points_last_iteration
            ):
                break

    # Missing points for deletion if required
    if delete_lost_points:

        # print(
        #     f&#34;{uw.mpi.rank} - Delete {len(not_my_points)} from swarm size {self.dm.getLocalSize()}&#34;,
        #     flush=True,
        # )

        uw.mpi.barrier()
        if len(not_my_points &gt; 0):
            indices = np.sort(not_my_points)[::-1]
            for index in indices:
                self.dm.removePointAtIndex(index)

        # print(
        #     f&#34;{uw.mpi.rank} - final swarm size {self.dm.getLocalSize()}&#34;,
        #     flush=True,
        # )

    return</code></pre>
</details>
<div class="desc"><p>Migrate swarm across processes after coordinates have been updated.</p>
<p>The algorithm uses a global kD-tree for the centroids of the domains to decide the particle mpi.rank (send to the closest)
If the particles are mis-assigned to a particular mpi.rank, the next choice is the second-closest and so on.</p>
<p>A few particles are still not found after this distribution process which probably means they are just outside the mesh.
If some points remain lost, they will be deleted if <code>delete_lost_points</code> is set.</p>
<p>Implementation note:
We retained (above) the name <code>DMSwarmPIC_coor</code> for the particle field to allow this routine to be inherited by a PIC swarm
which has this field pre-defined. (We'd need to add a cellid field as well, and re-compute it upon landing)</p></div>
</dd>
<dt id="underworld3.swarm.Swarm.petsc_save_checkpoint"><code class="name flex">
<span>def <span class="ident">petsc_save_checkpoint</span></span>(<span>self, swarmName: str, index: int, outputPath: str | None = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def petsc_save_checkpoint(
    self,
    swarmName: str,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
):
    &#34;&#34;&#34;

    Use PETSc to save the swarm and attached data to a .pbin and xdmf file.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    &#34;&#34;&#34;

    x_swarm_fname = f&#34;{outputPath}{swarmName}_{index:05d}.xmf&#34;
    self.dm.viewXDMF(x_swarm_fname)</code></pre>
</details>
<div class="desc"><p>Use PETSc to save the swarm and attached data to a .pbin and xdmf file.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.</p></div>
</dd>
<dt id="underworld3.swarm.Swarm.populate"><code class="name flex">
<span>def <span class="ident">populate</span></span>(<span>self, fill_param: int | None = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def populate(
    self,
    fill_param: Optional[int] = 1,
):
    &#34;&#34;&#34;
    Populate the swarm with particles throughout the domain.

    Parameters
    ----------
    fill_param:
        Parameter determining the particle count per cell (per dimension)
        for the given layout, using the mesh degree.
    &#34;&#34;&#34;

    self.fill_param = fill_param

    newp_coords0 = self.mesh._get_coords_for_basis(fill_param, continuous=False)
    newp_cells0 = self.mesh.get_closest_local_cells(newp_coords0)

    valid = newp_cells0 != -1
    newp_coords = newp_coords0[valid]
    newp_cells = newp_cells0[valid]

    self.dm.finalizeFieldRegister()
    self.dm.addNPoints(newp_coords.shape[0] + 1)

    coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))
    coords[...] = newp_coords[...]
    self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

    if self.recycle_rate &gt; 1:
        with self.access():
            # This is a mesh-local quantity, so let&#39;s just
            # store it on the mesh in an ad_hoc fashion for now

            self.mesh.particle_X_orig = self.particle_coordinates.data.copy()

        with self.access():
            swarm_orig_size = self.particle_coordinates.data.shape[0]
            all_local_coords = np.vstack(
                (self.particle_coordinates.data,) * (self.recycle_rate)
            )

            swarm_new_size = all_local_coords.data.shape[0]

        self.dm.addNPoints(swarm_new_size - swarm_orig_size)

        coords = self.dm.getField(&#34;DMSwarmPIC_coor&#34;).reshape((-1, self.dim))

        coords[...] = (
            all_local_coords[...]
            + (0.33 / (1 + fill_param))
            * (np.random.random(size=all_local_coords.shape) - 0.5)
            * 0.00001
            * self.mesh._search_lengths[all_local_cells]  # typical cell size
        )

        self.dm.restoreField(&#34;DMSwarmPIC_coor&#34;)

        ## Now set the cycle values

        with self.access(self._remeshed):
            for i in range(0, self.recycle_rate):
                offset = swarm_orig_size * i
                self._remeshed.data[offset::, 0] = i

    return</code></pre>
</details>
<div class="desc"><p>Populate the swarm with particles throughout the domain.</p>
<h2 id="parameters">Parameters</h2>
<p>fill_param:
Parameter determining the particle count per cell (per dimension)
for the given layout, using the mesh degree.</p></div>
</dd>
<dt id="underworld3.swarm.Swarm.read_timestep"><code class="name flex">
<span>def <span class="ident">read_timestep</span></span>(<span>self, base_filename: str, swarm_id: str, index: int, outputPath: str | None = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def read_timestep(
    self,
    base_filename: str,
    swarm_id: str,
    index: int,
    outputPath: Optional[str] = &#34;&#34;,
):
    output_base_name = os.path.join(outputPath, base_filename)
    swarm_file = output_base_name + f&#34;.{swarm_id}.{index:05}.h5&#34;

    ### open up file with coords on all procs
    with h5py.File(f&#34;{swarm_file}&#34;, &#34;r&#34;) as h5f:
        coordinates = h5f[&#34;coordinates&#34;][:]

    #### utilises the UW function for adding a swarm by an array
    self.add_particles_with_coordinates(coordinates)

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.Swarm.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self,<br>filename: int,<br>compression: bool | None = False,<br>compressionType: str | None = 'gzip',<br>force_sequential=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save(
    self,
    filename: int,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential=False,
):
    &#34;&#34;&#34;

    Save the swarm coordinates to a h5 file.

    Parameters
    ----------
    filename :
        The filename of the swarm checkpoint file to save to disk.
    compression :
        Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
    compressionType :
        Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.



    &#34;&#34;&#34;
    if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
        warnings.warn(
            &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
            stacklevel=2,
        )
    if filename.endswith(&#34;.h5&#34;) == False:
        raise RuntimeError(&#34;The filename must end with .h5&#34;)
    if compression == True and comm.rank == 0:
        warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)

    if h5py.h5.get_config().mpi == True and not force_sequential:
        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores. This is probably why the parallel
        # h5py write hangs

        with self.access():
            data_copy = self.data[:].copy()

        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
            if compression == True:
                h5f.create_dataset(
                    &#34;coordinates&#34;,
                    data=data_copy[:],
                    compression=compressionType,
                )
            else:
                h5f.create_dataset(&#34;coordinates&#34;, data=data_copy[:])

        del data_copy

    else:
        # It seems to be a bad idea to mix mpi barriers with the access
        # context manager so the copy-free version of this seems to hang
        # when there are many active cores

        with self.access():
            data_copy = self.data[:].copy()

        if comm.rank == 0:
            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                if compression == True:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                        compression=compressionType,
                    )
                else:
                    h5f.create_dataset(
                        &#34;coordinates&#34;,
                        data=data_copy,
                        chunks=True,
                        maxshape=(None, data_copy.shape[1]),
                    )

        comm.barrier()
        for i in range(1, comm.size):
            if comm.rank == i:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                    h5f[&#34;coordinates&#34;].resize(
                        (h5f[&#34;coordinates&#34;].shape[0] + data_copy.shape[0]),
                        axis=0,
                    )
                    # passive swarm, zero local particles is not unusual
                    if data_copy.shape[0] &gt; 0:
                        h5f[&#34;coordinates&#34;][-data_copy.shape[0] :] = data_copy[:]
            comm.barrier()
        comm.barrier()

        del data_copy

    return</code></pre>
</details>
<div class="desc"><p>Save the swarm coordinates to a h5 file.</p>
<h2 id="parameters">Parameters</h2>
<p>filename :
The filename of the swarm checkpoint file to save to disk.
compression :
Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
compressionType :
Type of compression to use, 'gzip' and 'lzf' supported. 'gzip' is default. Compression also needs to be set to 'True'.</p></div>
</dd>
<dt id="underworld3.swarm.Swarm.write_timestep"><code class="name flex">
<span>def <span class="ident">write_timestep</span></span>(<span>self,<br>filename: str,<br>swarmname: str,<br>index: int,<br>swarmVars: list | None = None,<br>outputPath: str | None = '',<br>time: int | None = None,<br>compression: bool | None = False,<br>compressionType: str | None = 'gzip',<br>force_sequential: bool | None = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def write_timestep(
    self,
    filename: str,
    swarmname: str,
    index: int,
    swarmVars: Optional[list] = None,
    outputPath: Optional[str] = &#34;&#34;,
    time: Optional[int] = None,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential: Optional[bool] = False,
):
    &#34;&#34;&#34;

    Save data to h5 and a corresponding xdmf for visualisation using h5py.

    Parameters
    ----------
    swarmName :
        Name of the swarm to save.
    swarmVars :
        List of swarm objects to save.
    index :
        An index which might correspond to the timestep or output number (for example).
    outputPath :
        Path to save the data. If left empty it will save the data in the current working directory.
    time :
        Attach the time to the generated xdmf.
    compression :
        Whether to compress the h5 files [bool].
    compressionType :
        The type of compression to use. &#39;gzip&#39; and &#39;lzf&#39; are the supported types, with &#39;gzip&#39; as the default.
    &#34;&#34;&#34;

    # This will eliminate the issue of whether or not to put path separators in the
    # outputPath. Also does the right thing if outputPath is &#34;&#34;

    output_base_name = os.path.join(outputPath, filename) + &#34;.&#34; + swarmname

    # check the directory where we will write checkpoint
    dir_path = os.path.dirname(output_base_name)  # get directory

    # check if path exists
    if os.path.exists(os.path.abspath(dir_path)):  # easier to debug abs
        pass
    else:
        raise RuntimeError(f&#34;{os.path.abspath(dir_path)} does not exist&#34;)

    # check if we have write access
    if os.access(os.path.abspath(dir_path), os.W_OK):
        pass
    else:
        raise RuntimeError(f&#34;No write access to {os.path.abspath(dir_path)}&#34;)

    # could also try to coerce this to be a list and raise if it fails (tuple, singleton ... )
    # also ... why the typechecking if this can still happen

    if swarmVars is not None and not isinstance(swarmVars, list):
        raise RuntimeError(&#34;`swarmVars` does not appear to be a list.&#34;)

    else:
        ### save the swarm particle location
        self.save(
            filename=f&#34;{output_base_name}.{index:05d}.h5&#34;,
            compression=compression,
            compressionType=compressionType,
            force_sequential=force_sequential,
        )

    #### Generate a h5 file for each field
    if swarmVars != None:
        for field in swarmVars:
            field.save(
                filename=f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;,
                compression=compression,
                compressionType=compressionType,
                force_sequential=force_sequential,
            )

    if uw.mpi.rank == 0:
        ### only need to combine the h5 files to a single xdmf on one proc
        with open(f&#34;{output_base_name}.{index:05d}.xdmf&#34;, &#34;w&#34;) as xdmf:
            # Write the XDMF header
            xdmf.write(&#39;&lt;?xml version=&#34;1.0&#34; ?&gt;\n&#39;)
            xdmf.write(
                &#39;&lt;Xdmf xmlns:xi=&#34;http://www.w3.org/2001/XInclude&#34; Version=&#34;2.0&#34;&gt;\n&#39;
            )
            xdmf.write(&#34;&lt;Domain&gt;\n&#34;)
            xdmf.write(
                f&#39;&lt;Grid Name=&#34;{output_base_name}.{index:05d}&#34; GridType=&#34;Uniform&#34;&gt;\n&#39;
            )

            if time != None:
                xdmf.write(f&#39;       &lt;Time Value=&#34;{time}&#34; /&gt;\n&#39;)

            # Write the grid element for the HDF5 dataset
            with h5py.File(f&#34;{output_base_name}.{index:05}.h5&#34;, &#34;r&#34;) as h5f:
                xdmf.write(
                    f&#39;      &lt;Topology Type=&#34;POLYVERTEX&#34; NodesPerElement=&#34;{h5f[&#34;coordinates&#34;].shape[0]}&#34;&gt; &lt;/Topology&gt;\n&#39;
                )
                if h5f[&#34;coordinates&#34;].shape[1] == 2:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XY&#34;&gt;\n&#39;)
                elif h5f[&#34;coordinates&#34;].shape[1] == 3:
                    xdmf.write(&#39;            &lt;Geometry Type=&#34;XYZ&#34;&gt;\n&#39;)
                xdmf.write(
                    f&#39;                      &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;coordinates&#34;].shape[0]} {h5f[&#34;coordinates&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/coordinates&lt;/DataItem&gt;\n&#39;
                )
                xdmf.write(&#34;                &lt;/Geometry&gt;\n&#34;)

            # Write the attribute element for the field
            if swarmVars != None:
                for field in swarmVars:
                    with h5py.File(
                        f&#34;{output_base_name}.{field.name}.{index:05d}.h5&#34;, &#34;r&#34;
                    ) as h5f:
                        if h5f[&#34;data&#34;].dtype == np.int32:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Int&#34; Precision=&#34;4&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 1:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Scalar&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        elif h5f[&#34;data&#34;].shape[1] == 2 or h5f[&#34;data&#34;].shape[1] == 3:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Vector&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )
                        else:
                            xdmf.write(
                                f&#39;  &lt;Attribute Type=&#34;Tensor&#34; Center=&#34;Node&#34; Name=&#34;{field.name}&#34;&gt;\n&#39;
                            )
                            xdmf.write(
                                f&#39;                  &lt;DataItem Format=&#34;HDF&#34; NumberType=&#34;Float&#34; Precision=&#34;8&#34; Dimensions=&#34;{h5f[&#34;data&#34;].shape[0]} {h5f[&#34;data&#34;].shape[1]}&#34;&gt;{os.path.basename(h5f.filename)}:/data&lt;/DataItem&gt;\n&#39;
                            )

                        xdmf.write(&#34;        &lt;/Attribute&gt;\n&#34;)
            else:
                pass

            # Write the XDMF footer
            xdmf.write(&#34;&lt;/Grid&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Domain&gt;\n&#34;)
            xdmf.write(&#34;&lt;/Xdmf&gt;\n&#34;)</code></pre>
</details>
<div class="desc"><p>Save data to h5 and a corresponding xdmf for visualisation using h5py.</p>
<h2 id="parameters">Parameters</h2>
<p>swarmName :
Name of the swarm to save.
swarmVars :
List of swarm objects to save.
index :
An index which might correspond to the timestep or output number (for example).
outputPath :
Path to save the data. If left empty it will save the data in the current working directory.
time :
Attach the time to the generated xdmf.
compression :
Whether to compress the h5 files [bool].
compressionType :
The type of compression to use. 'gzip' and 'lzf' are the supported types, with 'gzip' as the default.</p></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout"><code class="flex name class">
<span>class <span class="ident">SwarmPICLayout</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmPICLayout(Enum):
    &#34;&#34;&#34;
    Particle population fill type:

    SwarmPICLayout.REGULAR     defines points on a regular ijk mesh. Supported by simplex cell types only.
    SwarmPICLayout.GAUSS       defines points using an npoint Gauss-Legendre tensor product quadrature rule.
    SwarmPICLayout.SUBDIVISION defines points on the centroid of a sub-divided reference cell.
    &#34;&#34;&#34;

    REGULAR = 0
    GAUSS = 1
    SUBDIVISION = 2</code></pre>
</details>
<div class="desc"><p>Particle population fill type:</p>
<p>SwarmPICLayout.REGULAR
defines points on a regular ijk mesh. Supported by simplex cell types only.
SwarmPICLayout.GAUSS
defines points using an npoint Gauss-Legendre tensor product quadrature rule.
SwarmPICLayout.SUBDIVISION defines points on the centroid of a sub-divided reference cell.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmPICLayout.GAUSS"><code class="name">var <span class="ident">GAUSS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout.REGULAR"><code class="name">var <span class="ident">REGULAR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmPICLayout.SUBDIVISION"><code class="name">var <span class="ident">SUBDIVISION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmType"><code class="flex name class">
<span>class <span class="ident">SwarmType</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmType(Enum):
    DMSWARM_BASIC = 0
    DMSWARM_PIC = 1</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access::</li>
</ul>
<pre><code class="language-python-repl">&gt;&gt;&gt; Color.RED
&lt;Color.RED: 1&gt;
</code></pre>
<ul>
<li>value lookup:</li>
</ul>
<pre><code class="language-python-repl">&gt;&gt;&gt; Color(1)
&lt;Color.RED: 1&gt;
</code></pre>
<ul>
<li>name lookup:</li>
</ul>
<pre><code class="language-python-repl">&gt;&gt;&gt; Color['RED']
&lt;Color.RED: 1&gt;
</code></pre>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmType.DMSWARM_BASIC"><code class="name">var <span class="ident">DMSWARM_BASIC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmType.DMSWARM_PIC"><code class="name">var <span class="ident">DMSWARM_PIC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="underworld3.swarm.SwarmVariable"><code class="flex name class">
<span>class <span class="ident">SwarmVariable</span></span>
<span>(</span><span>name,<br>swarm,<br>size=None,<br>vtype=None,<br>dtype=builtins.float,<br>proxy_degree=1,<br>proxy_continuous=True,<br>varsymbol=None,<br>rebuild_on_cycle=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SwarmVariable(Stateful, uw_object):
    &#34;&#34;&#34;
    The SwarmVariable class generates a variable supported by a point cloud or &#39;swarm&#39; and the
    underlying meshVariable representation that makes it possible to construct expressions that
    depend on the values of the swarmVariable.

    To set / read nodal values, use the numpy interface via the &#39;data&#39; property.

    Parameters
    ----------
    varname :
        A textual name for this variable.
    swarm :
        The supporting underworld swarm.
    size :
        The shape of a Matrix variable type.
    vtype :
        Semi-Optional. The underworld variable type for this variable.
    proxy_degree :
        The polynomial degree for this variable.
    proxy_continuous :
        The polynomial degree for this variable.
    varsymbol:
        A symbolic form for printing etc (sympy / latex)
    rebuild_on_cycle:
        For cyclic swarm variables — True is the best choice for continuous fields

    &#34;&#34;&#34;

    @timing.routine_timer_decorator
    def __init__(
        self,
        name,
        swarm,
        size=None,  # only needed if MATRIX type
        vtype=None,
        dtype=float,
        proxy_degree=1,
        proxy_continuous=True,
        _register=True,
        _proxy=True,
        _nn_proxy=False,
        varsymbol=None,
        rebuild_on_cycle=True,
    ):
        if name in swarm.vars.keys():
            raise ValueError(
                &#34;Variable with name {} already exists on swarm.&#34;.format(name)
            )

        import re
        import sympy
        import math

        if varsymbol is None:
            varsymbol = name

        self.name = name
        self.clean_name = re.sub(r&#34;[^a-zA-Z0-9_]&#34;, &#34;&#34;, name)
        self.symbol = varsymbol

        self.swarm = swarm
        self.shape = size

        mesh = swarm.mesh

        if vtype == None:
            if isinstance(size, int) and size == 1:
                vtype = uw.VarType.SCALAR
            elif isinstance(size, int) and size == mesh.dim:
                vtype = uw.VarType.VECTOR
            elif isinstance(size, tuple):
                if size[0] == mesh.dim and size[1] == mesh.dim:
                    vtype = uw.VarType.TENSOR
                else:
                    vtype = uw.VarType.MATRIX
            else:
                raise ValueError(
                    &#34;Unable to infer variable type from `num_components`. Please explicitly set the `vtype` parameter.&#34;
                )

        self.vtype = vtype

        if not isinstance(vtype, uw.VarType):
            raise ValueError(
                &#34;&#39;vtype&#39; must be an instance of &#39;Variable_Type&#39;, for example `underworld.VarType.SCALAR`.&#34;
            )

        if vtype == uw.VarType.SCALAR:
            self.num_components = 1
            self.shape = (1, 1)
            self.cpt_map = 0
        elif vtype == uw.VarType.VECTOR:
            self.num_components = mesh.dim
            self.shape = (1, mesh.dim)
            self.cpt_map = tuple(range(0, mesh.dim))
        elif vtype == uw.VarType.TENSOR:
            self.num_components = mesh.dim * mesh.dim
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.SYM_TENSOR:
            self.num_components = math.comb(mesh.dim + 1, 2)
            self.shape = (mesh.dim, mesh.dim)
        elif vtype == uw.VarType.MATRIX:
            self.num_components = self.shape[0] * self.shape[1]

        self._data_container = np.empty(self.shape, dtype=object)

        if (dtype == float) or (dtype == &#34;float&#34;) or (dtype == np.float64):
            self.dtype = float
            petsc_type = PETSc.ScalarType
        elif (
            (dtype == int)
            or (dtype == &#34;int&#34;)
            or (dtype == np.int32)
            or (dtype == np.int64)
        ):
            self.dtype = int
            petsc_type = PETSc.IntType
        else:
            raise TypeError(
                f&#34;Provided dtype={dtype} is not supported. Supported types are &#39;int&#39; and &#39;float&#39;.&#34;
            )

        if _register:
            self.swarm.dm.registerField(
                self.clean_name, self.num_components, dtype=petsc_type
            )

        self._data = None
        # add to swarms dict

        self.swarm._vars[self.clean_name] = self
        self._is_accessed = False

        # proxy variable
        self._proxy = _proxy
        self._vtype = vtype
        self._proxy_degree = proxy_degree
        self._proxy_continuous = proxy_continuous
        self._nn_proxy = _nn_proxy
        self._create_proxy_variable()

        # recycle swarm
        self._rebuild_on_cycle = rebuild_on_cycle
        self._register = _register

        from collections import namedtuple

        SwarmVariable_ij = namedtuple(&#34;SwarmVariable_ij&#34;, [&#34;data&#34;, &#34;sym&#34;])

        if self._proxy:
            for i in range(0, self.shape[0]):
                for j in range(0, self.shape[1]):
                    self._data_container[i, j] = SwarmVariable_ij(
                        data=f&#34;SwarmVariable[...].data is only available within mesh.access() context&#34;,
                        sym=self.sym[i, j],
                    )

        super().__init__()

        return

    def __getitem__(self, indices):
        if not isinstance(indices, tuple):
            if isinstance(indices, int) and self.shape[0] == 1:
                i = 0
                j = indices
            else:
                raise IndexError(
                    &#34;SwarmVariable[i,j] access requires one or two indices &#34;
                )
        else:
            i, j = indices

        return self._data_container[i, j]

    ## Should be a single master copy
    def _data_layout(self, i, j=None):
        # mapping

        if self.vtype == uw.VarType.SCALAR:
            return 0
        if self.vtype == uw.VarType.VECTOR:
            if j is None:
                return i
            elif i == 0:
                return j
            else:
                raise IndexError(
                    f&#34;Vectors have shape {self.mesh.dim} or {(1, self.mesh.dim)} &#34;
                )
        if self.vtype == uw.VarType.TENSOR:
            if self.swarm.mesh.dim == 2:
                return ((0, 1), (2, 3))[i][j]
            else:
                return ((0, 1, 2), (3, 4, 5), (6, 7, 8))[i][j]

        if self.vtype == uw.VarType.SYM_TENSOR:
            if self.swarm.mesh.dim == 2:
                return ((0, 2), (2, 1))[i][j]
            else:
                return ((0, 3, 4), (3, 1, 5), (4, 5, 2))[i][j]

        if self.vtype == uw.VarType.MATRIX:
            return i + j * self.shape[0]

    def _create_proxy_variable(self):
        # release if defined
        self._meshVar = None

        if self._proxy:
            self._meshVar = uw.discretisation.MeshVariable(
                &#34;proxy_&#34; + self.clean_name,
                self.swarm._mesh,
                self.shape,
                self._vtype,
                degree=self._proxy_degree,
                continuous=self._proxy_continuous,
                varsymbol=r&#34;\left&lt;&#34; + self.symbol + r&#34;\right&gt;&#34;,
            )

    def _update(self):
        &#34;&#34;&#34;
        This method updates the proxy mesh variable for the current
        swarm &amp; particle variable state.
        &#34;&#34;&#34;

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            return

        else:
            self._rbf_to_meshVar(self._meshVar)

        return

    # Maybe rbf_interpolate for this one and meshVar is a special case
    def _rbf_to_meshVar(self, meshVar, nnn=None, verbose=False):
        &#34;&#34;&#34;
        Here is how it works: for each particle, create a distance-weighted average on the node data

        Todo: caching the k-d trees etc for the proxy-mesh-variable nodal points
        Todo: some form of global fall-back for when there are no particles on a processor
        &#34;&#34;&#34;

        # Mapping to the coordinates of the variable from the
        # particle coords

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        if meshVar.mesh != self.swarm.mesh:
            raise RuntimeError(&#34;Cannot map a swarm to a different mesh&#34;)

        new_coords = meshVar.coords

        Values = self.rbf_interpolate(new_coords, verbose=verbose, nnn=nnn)

        with meshVar.mesh.access(meshVar):
            meshVar.data[...] = Values[...]

        return

    def _rbf_reduce_to_meshVar(self, meshVar, verbose=False):
        &#34;&#34;&#34;
        This method updates a mesh variable for the current
        swarm &amp; particle variable state by reducing the swarm to
        the nearest point for each particle

        Here is how it works:

            1) for each particle, create a distance-weighted average on the node data
            2) check to see which nodes have zero weight / zero contribution and replace with nearest particle value

        Todo: caching the k-d trees etc for the proxy-mesh-variable nodal points
        Todo: some form of global fall-back for when there are no particles on a processor

        &#34;&#34;&#34;

        # if not proxied, nothing to do. return.
        if not self._meshVar:
            return

        # 1 - Average particles to nodes with distance weighted average

        kd = uw.kdtree.KDTree(meshVar.coords)

        with self.swarm.access():
            d, n = kd.query(self.swarm.data, k=1)

            node_values = np.zeros((meshVar.coords.shape[0], self.num_components))
            w = np.zeros(meshVar.coords.shape[0])

            if not self._nn_proxy:
                for i in range(self.data.shape[0]):
                    # if b[i]:
                    node_values[n[i], :] += self.data[i, :] / (1.0e-24 + d[i])
                    w[n[i]] += 1.0 / (1.0e-24 + d[i])

                node_values[np.where(w &gt; 0.0)[0], :] /= w[np.where(w &gt; 0.0)[0]].reshape(
                    -1, 1
                )

        # 2 - set NN vals on mesh var where w == 0.0

        p_nnmap = self.swarm._get_map(self)

        with self.swarm.mesh.access(meshVar), self.swarm.access():
            meshVar.data[...] = node_values[...]
            meshVar.data[np.where(w == 0.0), :] = self.data[
                p_nnmap[np.where(w == 0.0)], :
            ]

        return

    def rbf_interpolate(self, new_coords, verbose=False, nnn=None):
        # An inverse-distance mapping is quite robust here ... as long
        # as we take care of the case where some nodes coincide (likely if used with mesh2mesh)
        # We try to eliminate contributions from recently remeshed particles

        import numpy as np

        with self.swarm.access():
            data_size = self.data.shape

        # What to do if there are no particles
        if data_size[0] &lt;= 1:
            return np.zeros((new_coords.shape[0], data_size[1]))

        if nnn is None:
            nnn = self.swarm.mesh.dim + 1

        if nnn &gt; data_size[0]:
            nnn = data_size[0]

        with self.swarm.access():
            if self.swarm.recycle_rate &gt; 1:
                not_remeshed = self.swarm._remeshed.data[:, 0] != 0
                D = self.data[not_remeshed].copy()

                kdt = uw.kdtree.KDTree(
                    self.swarm.particle_coordinates.data[not_remeshed, :]
                )
            else:
                D = self.data.copy()
                kdt = uw.kdtree.KDTree(self.swarm.particle_coordinates.data[:, :])

            # kdt.build_index()

            values = kdt.rbf_interpolator_local(new_coords, D, nnn, 2, verbose)

            del kdt

        return values

    @property
    def data(self):
        if self._data is None:
            raise RuntimeError(
                &#34;Data must be accessed via the swarm `access()` context manager.&#34;
            )
        return self._data

    @property
    def sym(self):
        return self._meshVar.sym

    @property
    def sym_1d(self):
        return self._meshVar.sym_1d

    @timing.routine_timer_decorator
    def save(
        self,
        filename: int,
        compression: Optional[bool] = False,
        compressionType: Optional[str] = &#34;gzip&#34;,
        force_sequential=False,
    ):
        &#34;&#34;&#34;

        Save the swarm variable to a h5 file.

        Parameters
        ----------
        filename :
            The filename of the swarm variable to save to disk.
        compression :
            Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
        compressionType :
            Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.

        force_sequential : activate the serial version of hdf5

        &#34;&#34;&#34;
        if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
            warnings.warn(
                &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
                stacklevel=2,
            )
        if compression == True and comm.rank == 0:
            warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)
        if filename.endswith(&#34;.h5&#34;) == False:
            raise RuntimeError(&#34;The filename must end with .h5&#34;)

        if h5py.h5.get_config().mpi == True and not force_sequential:
            with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
                with self.swarm.access(self):
                    if compression == True:
                        h5f.create_dataset(
                            &#34;data&#34;, data=self.data[:], compression=compressionType
                        )
                    else:
                        h5f.create_dataset(&#34;data&#34;, data=self.data[:])
        else:
            with self.swarm.access(self):
                if comm.rank == 0:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                        if compression == True:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                                compression=compressionType,
                            )
                        else:
                            h5f.create_dataset(
                                &#34;data&#34;,
                                data=self.data[:],
                                chunks=True,
                                maxshape=(None, self.data.shape[1]),
                            )
                comm.barrier()
                for proc in range(1, comm.size):
                    if comm.rank == proc:
                        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                            h5f[&#34;data&#34;].resize(
                                (h5f[&#34;data&#34;].shape[0] + self.data.shape[0]), axis=0
                            )
                            h5f[&#34;data&#34;][-self.data.shape[0] :] = self.data[:]
                    comm.barrier()
                comm.barrier()

        return

    @timing.routine_timer_decorator
    def write_proxy(self, filename: str):
        # if not proxied, nothing to do. return.
        if not self._meshVar:
            if uw.mpi.rank == 0:
                print(&#34;No proxy mesh variable that can be saved&#34;, flush=True)
            return

        self._meshVar.write(filename)

        return

    @timing.routine_timer_decorator
    def read_timestep(
        self,
        data_filename: str,
        swarmID: str,
        data_name: str,
        index: int,
        outputPath=&#34;&#34;,
    ):
        # mesh.write_timestep( &#34;test&#34;, meshUpdates=False, meshVars=[X], outputPath=&#34;&#34;, index=0)
        # swarm.write_timestep(&#34;test&#34;, &#34;swarm&#34;, swarmVars=[var], outputPath=&#34;&#34;, index=0)

        output_base_name = os.path.join(outputPath, data_filename)
        swarmFilename = output_base_name + f&#34;.{swarmID}.{index:05}.h5&#34;
        filename = output_base_name + f&#34;.{swarmID}.{data_name}.{index:05}.h5&#34;

        # check if swarmFilename exists
        if os.path.isfile(os.path.abspath(swarmFilename)):  # easier to debug abs path
            print(f&#34;Reading swarm information from {swarmFilename}&#34;, flush=True)
            pass
        else:
            raise RuntimeError(f&#34;{os.path.abspath(swarmFilename)} does not exist&#34;)

        if os.path.isfile(os.path.abspath(filename)):
            print(f&#34;Reading variable information from {filename}&#34;, flush=True)

            pass
        else:
            raise RuntimeError(f&#34;{os.path.abspath(filename)} does not exist&#34;)

        ### open up file with coords on all procs and open up data on all procs. May be problematic for large problems.
        with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f_data, h5py.File(
            f&#34;{swarmFilename}&#34;, &#34;r&#34;
        ) as h5f_swarm:
            with self.swarm.access(self):
                var_dtype = self.data.dtype
                file_dtype = h5f_data[&#34;data&#34;][:].dtype
                file_length = h5f_data[&#34;data&#34;][:].shape[0]

                if var_dtype != file_dtype:
                    if comm.rank == 0:
                        warnings.warn(
                            f&#34;{os.path.basename(filename)} dtype ({file_dtype}) does not match {self.name} swarm variable dtype ({var_dtype}) which may result in a loss of data.&#34;,
                            stacklevel=2,
                        )

                # First work out which are local points and ignore the rest
                # This might help speed up the load by dropping lots of particles

                all_coords = h5f_swarm[&#34;coordinates&#34;][()]
                all_data = h5f_data[&#34;data&#34;][()]

                # cell = self.swarm.mesh.get_closest_local_cells(all_coords)
                # local = np.where(cell &gt;= 0)[0]
                # # not_not_local = np.where(cell == -1)[0]

                local_coords = all_coords  # [local]
                local_data = all_data  # [local]

                kdt = uw.kdtree.KDTree(local_coords)

                self.data[:] = kdt.rbf_interpolator_local(
                    self.swarm.data, local_data, nnn=1
                )

        return</code></pre>
</details>
<div class="desc"><p>The SwarmVariable class generates a variable supported by a point cloud or 'swarm' and the
underlying meshVariable representation that makes it possible to construct expressions that
depend on the values of the swarmVariable.</p>
<p>To set / read nodal values, use the numpy interface via the 'data' property.</p>
<h2 id="parameters">Parameters</h2>
<p>varname :
A textual name for this variable.
swarm :
The supporting underworld swarm.
size :
The shape of a Matrix variable type.
vtype :
Semi-Optional. The underworld variable type for this variable.
proxy_degree :
The polynomial degree for this variable.
proxy_continuous :
The polynomial degree for this variable.
varsymbol:
A symbolic form for printing etc (sympy / latex)
rebuild_on_cycle:
For cyclic swarm variables — True is the best choice for continuous fields</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>underworld3.utilities._api_tools.Stateful</li>
<li>underworld3.utilities._api_tools.uw_object</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="underworld3.swarm.IndexSwarmVariable" href="#underworld3.swarm.IndexSwarmVariable">IndexSwarmVariable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="underworld3.swarm.SwarmVariable.data"><code class="name">prop <span class="ident">data</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self):
    if self._data is None:
        raise RuntimeError(
            &#34;Data must be accessed via the swarm `access()` context manager.&#34;
        )
    return self._data</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmVariable.sym"><code class="name">prop <span class="ident">sym</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym(self):
    return self._meshVar.sym</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmVariable.sym_1d"><code class="name">prop <span class="ident">sym_1d</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sym_1d(self):
    return self._meshVar.sym_1d</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="underworld3.swarm.SwarmVariable.rbf_interpolate"><code class="name flex">
<span>def <span class="ident">rbf_interpolate</span></span>(<span>self, new_coords, verbose=False, nnn=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rbf_interpolate(self, new_coords, verbose=False, nnn=None):
    # An inverse-distance mapping is quite robust here ... as long
    # as we take care of the case where some nodes coincide (likely if used with mesh2mesh)
    # We try to eliminate contributions from recently remeshed particles

    import numpy as np

    with self.swarm.access():
        data_size = self.data.shape

    # What to do if there are no particles
    if data_size[0] &lt;= 1:
        return np.zeros((new_coords.shape[0], data_size[1]))

    if nnn is None:
        nnn = self.swarm.mesh.dim + 1

    if nnn &gt; data_size[0]:
        nnn = data_size[0]

    with self.swarm.access():
        if self.swarm.recycle_rate &gt; 1:
            not_remeshed = self.swarm._remeshed.data[:, 0] != 0
            D = self.data[not_remeshed].copy()

            kdt = uw.kdtree.KDTree(
                self.swarm.particle_coordinates.data[not_remeshed, :]
            )
        else:
            D = self.data.copy()
            kdt = uw.kdtree.KDTree(self.swarm.particle_coordinates.data[:, :])

        # kdt.build_index()

        values = kdt.rbf_interpolator_local(new_coords, D, nnn, 2, verbose)

        del kdt

    return values</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmVariable.read_timestep"><code class="name flex">
<span>def <span class="ident">read_timestep</span></span>(<span>self, data_filename: str, swarmID: str, data_name: str, index: int, outputPath='')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def read_timestep(
    self,
    data_filename: str,
    swarmID: str,
    data_name: str,
    index: int,
    outputPath=&#34;&#34;,
):
    # mesh.write_timestep( &#34;test&#34;, meshUpdates=False, meshVars=[X], outputPath=&#34;&#34;, index=0)
    # swarm.write_timestep(&#34;test&#34;, &#34;swarm&#34;, swarmVars=[var], outputPath=&#34;&#34;, index=0)

    output_base_name = os.path.join(outputPath, data_filename)
    swarmFilename = output_base_name + f&#34;.{swarmID}.{index:05}.h5&#34;
    filename = output_base_name + f&#34;.{swarmID}.{data_name}.{index:05}.h5&#34;

    # check if swarmFilename exists
    if os.path.isfile(os.path.abspath(swarmFilename)):  # easier to debug abs path
        print(f&#34;Reading swarm information from {swarmFilename}&#34;, flush=True)
        pass
    else:
        raise RuntimeError(f&#34;{os.path.abspath(swarmFilename)} does not exist&#34;)

    if os.path.isfile(os.path.abspath(filename)):
        print(f&#34;Reading variable information from {filename}&#34;, flush=True)

        pass
    else:
        raise RuntimeError(f&#34;{os.path.abspath(filename)} does not exist&#34;)

    ### open up file with coords on all procs and open up data on all procs. May be problematic for large problems.
    with h5py.File(f&#34;{filename}&#34;, &#34;r&#34;) as h5f_data, h5py.File(
        f&#34;{swarmFilename}&#34;, &#34;r&#34;
    ) as h5f_swarm:
        with self.swarm.access(self):
            var_dtype = self.data.dtype
            file_dtype = h5f_data[&#34;data&#34;][:].dtype
            file_length = h5f_data[&#34;data&#34;][:].shape[0]

            if var_dtype != file_dtype:
                if comm.rank == 0:
                    warnings.warn(
                        f&#34;{os.path.basename(filename)} dtype ({file_dtype}) does not match {self.name} swarm variable dtype ({var_dtype}) which may result in a loss of data.&#34;,
                        stacklevel=2,
                    )

            # First work out which are local points and ignore the rest
            # This might help speed up the load by dropping lots of particles

            all_coords = h5f_swarm[&#34;coordinates&#34;][()]
            all_data = h5f_data[&#34;data&#34;][()]

            # cell = self.swarm.mesh.get_closest_local_cells(all_coords)
            # local = np.where(cell &gt;= 0)[0]
            # # not_not_local = np.where(cell == -1)[0]

            local_coords = all_coords  # [local]
            local_data = all_data  # [local]

            kdt = uw.kdtree.KDTree(local_coords)

            self.data[:] = kdt.rbf_interpolator_local(
                self.swarm.data, local_data, nnn=1
            )

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="underworld3.swarm.SwarmVariable.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self,<br>filename: int,<br>compression: bool | None = False,<br>compressionType: str | None = 'gzip',<br>force_sequential=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def save(
    self,
    filename: int,
    compression: Optional[bool] = False,
    compressionType: Optional[str] = &#34;gzip&#34;,
    force_sequential=False,
):
    &#34;&#34;&#34;

    Save the swarm variable to a h5 file.

    Parameters
    ----------
    filename :
        The filename of the swarm variable to save to disk.
    compression :
        Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
    compressionType :
        Type of compression to use, &#39;gzip&#39; and &#39;lzf&#39; supported. &#39;gzip&#39; is default. Compression also needs to be set to &#39;True&#39;.

    force_sequential : activate the serial version of hdf5

    &#34;&#34;&#34;
    if h5py.h5.get_config().mpi == False and comm.size &gt; 1 and comm.rank == 0:
        warnings.warn(
            &#34;Collective IO not possible as h5py not available in parallel mode. Switching to sequential. This will be slow for models running on multiple processors&#34;,
            stacklevel=2,
        )
    if compression == True and comm.rank == 0:
        warnings.warn(&#34;Compression may slow down write times&#34;, stacklevel=2)
    if filename.endswith(&#34;.h5&#34;) == False:
        raise RuntimeError(&#34;The filename must end with .h5&#34;)

    if h5py.h5.get_config().mpi == True and not force_sequential:
        with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;, driver=&#34;mpio&#34;, comm=comm) as h5f:
            with self.swarm.access(self):
                if compression == True:
                    h5f.create_dataset(
                        &#34;data&#34;, data=self.data[:], compression=compressionType
                    )
                else:
                    h5f.create_dataset(&#34;data&#34;, data=self.data[:])
    else:
        with self.swarm.access(self):
            if comm.rank == 0:
                with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;w&#34;) as h5f:
                    if compression == True:
                        h5f.create_dataset(
                            &#34;data&#34;,
                            data=self.data[:],
                            chunks=True,
                            maxshape=(None, self.data.shape[1]),
                            compression=compressionType,
                        )
                    else:
                        h5f.create_dataset(
                            &#34;data&#34;,
                            data=self.data[:],
                            chunks=True,
                            maxshape=(None, self.data.shape[1]),
                        )
            comm.barrier()
            for proc in range(1, comm.size):
                if comm.rank == proc:
                    with h5py.File(f&#34;{filename[:-3]}.h5&#34;, &#34;a&#34;) as h5f:
                        h5f[&#34;data&#34;].resize(
                            (h5f[&#34;data&#34;].shape[0] + self.data.shape[0]), axis=0
                        )
                        h5f[&#34;data&#34;][-self.data.shape[0] :] = self.data[:]
                comm.barrier()
            comm.barrier()

    return</code></pre>
</details>
<div class="desc"><p>Save the swarm variable to a h5 file.</p>
<h2 id="parameters">Parameters</h2>
<p>filename :
The filename of the swarm variable to save to disk.
compression :
Add compression to the h5 files (saves space but increases write times with increasing no. of processors)
compressionType :
Type of compression to use, 'gzip' and 'lzf' supported. 'gzip' is default. Compression also needs to be set to 'True'.</p>
<dl>
<dt><strong><code>force_sequential</code></strong> :&ensp;<code>activate the serial version</code> of <code>hdf5</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
<dt id="underworld3.swarm.SwarmVariable.write_proxy"><code class="name flex">
<span>def <span class="ident">write_proxy</span></span>(<span>self, filename: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timing.routine_timer_decorator
def write_proxy(self, filename: str):
    # if not proxied, nothing to do. return.
    if not self._meshVar:
        if uw.mpi.rank == 0:
            print(&#34;No proxy mesh variable that can be saved&#34;, flush=True)
        return

    self._meshVar.write(filename)

    return</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="underworld3" href="index.html">underworld3</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="underworld3.swarm.IndexSwarmVariable" href="#underworld3.swarm.IndexSwarmVariable">IndexSwarmVariable</a></code></h4>
<ul class="two-column">
<li><code><a title="underworld3.swarm.IndexSwarmVariable.createMask" href="#underworld3.swarm.IndexSwarmVariable.createMask">createMask</a></code></li>
<li><code><a title="underworld3.swarm.IndexSwarmVariable.sym" href="#underworld3.swarm.IndexSwarmVariable.sym">sym</a></code></li>
<li><code><a title="underworld3.swarm.IndexSwarmVariable.sym_1d" href="#underworld3.swarm.IndexSwarmVariable.sym_1d">sym_1d</a></code></li>
<li><code><a title="underworld3.swarm.IndexSwarmVariable.view" href="#underworld3.swarm.IndexSwarmVariable.view">view</a></code></li>
<li><code><a title="underworld3.swarm.IndexSwarmVariable.viewMask" href="#underworld3.swarm.IndexSwarmVariable.viewMask">viewMask</a></code></li>
<li><code><a title="underworld3.swarm.IndexSwarmVariable.visMask" href="#underworld3.swarm.IndexSwarmVariable.visMask">visMask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.NodalPointPICSwarm" href="#underworld3.swarm.NodalPointPICSwarm">NodalPointPICSwarm</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.NodalPointPICSwarm.advection" href="#underworld3.swarm.NodalPointPICSwarm.advection">advection</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.NodalPointUWSwarm" href="#underworld3.swarm.NodalPointUWSwarm">NodalPointUWSwarm</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.NodalPointUWSwarm.advection" href="#underworld3.swarm.NodalPointUWSwarm.advection">advection</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.PICSwarm" href="#underworld3.swarm.PICSwarm">PICSwarm</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.PICSwarm.access" href="#underworld3.swarm.PICSwarm.access">access</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.add_particles_with_coordinates" href="#underworld3.swarm.PICSwarm.add_particles_with_coordinates">add_particles_with_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.add_variable" href="#underworld3.swarm.PICSwarm.add_variable">add_variable</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.advection" href="#underworld3.swarm.PICSwarm.advection">advection</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.data" href="#underworld3.swarm.PICSwarm.data">data</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.estimate_dt" href="#underworld3.swarm.PICSwarm.estimate_dt">estimate_dt</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.instances" href="#underworld3.swarm.PICSwarm.instances">instances</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.mesh" href="#underworld3.swarm.PICSwarm.mesh">mesh</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.particle_cellid" href="#underworld3.swarm.PICSwarm.particle_cellid">particle_cellid</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.particle_coordinates" href="#underworld3.swarm.PICSwarm.particle_coordinates">particle_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.petsc_save_checkpoint" href="#underworld3.swarm.PICSwarm.petsc_save_checkpoint">petsc_save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.populate" href="#underworld3.swarm.PICSwarm.populate">populate</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.populate_petsc" href="#underworld3.swarm.PICSwarm.populate_petsc">populate_petsc</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.read_timestep" href="#underworld3.swarm.PICSwarm.read_timestep">read_timestep</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.save" href="#underworld3.swarm.PICSwarm.save">save</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.vars" href="#underworld3.swarm.PICSwarm.vars">vars</a></code></li>
<li><code><a title="underworld3.swarm.PICSwarm.write_timestep" href="#underworld3.swarm.PICSwarm.write_timestep">write_timestep</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.Swarm" href="#underworld3.swarm.Swarm">Swarm</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.Swarm.access" href="#underworld3.swarm.Swarm.access">access</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_particles_with_coordinates" href="#underworld3.swarm.Swarm.add_particles_with_coordinates">add_particles_with_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_particles_with_global_coordinates" href="#underworld3.swarm.Swarm.add_particles_with_global_coordinates">add_particles_with_global_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.add_variable" href="#underworld3.swarm.Swarm.add_variable">add_variable</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.advection" href="#underworld3.swarm.Swarm.advection">advection</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.data" href="#underworld3.swarm.Swarm.data">data</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.estimate_dt" href="#underworld3.swarm.Swarm.estimate_dt">estimate_dt</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.instances" href="#underworld3.swarm.Swarm.instances">instances</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.mesh" href="#underworld3.swarm.Swarm.mesh">mesh</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.migrate" href="#underworld3.swarm.Swarm.migrate">migrate</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.particle_coordinates" href="#underworld3.swarm.Swarm.particle_coordinates">particle_coordinates</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.petsc_save_checkpoint" href="#underworld3.swarm.Swarm.petsc_save_checkpoint">petsc_save_checkpoint</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.populate" href="#underworld3.swarm.Swarm.populate">populate</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.read_timestep" href="#underworld3.swarm.Swarm.read_timestep">read_timestep</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.save" href="#underworld3.swarm.Swarm.save">save</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.vars" href="#underworld3.swarm.Swarm.vars">vars</a></code></li>
<li><code><a title="underworld3.swarm.Swarm.write_timestep" href="#underworld3.swarm.Swarm.write_timestep">write_timestep</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmPICLayout" href="#underworld3.swarm.SwarmPICLayout">SwarmPICLayout</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.SwarmPICLayout.GAUSS" href="#underworld3.swarm.SwarmPICLayout.GAUSS">GAUSS</a></code></li>
<li><code><a title="underworld3.swarm.SwarmPICLayout.REGULAR" href="#underworld3.swarm.SwarmPICLayout.REGULAR">REGULAR</a></code></li>
<li><code><a title="underworld3.swarm.SwarmPICLayout.SUBDIVISION" href="#underworld3.swarm.SwarmPICLayout.SUBDIVISION">SUBDIVISION</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmType" href="#underworld3.swarm.SwarmType">SwarmType</a></code></h4>
<ul class="">
<li><code><a title="underworld3.swarm.SwarmType.DMSWARM_BASIC" href="#underworld3.swarm.SwarmType.DMSWARM_BASIC">DMSWARM_BASIC</a></code></li>
<li><code><a title="underworld3.swarm.SwarmType.DMSWARM_PIC" href="#underworld3.swarm.SwarmType.DMSWARM_PIC">DMSWARM_PIC</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="underworld3.swarm.SwarmVariable" href="#underworld3.swarm.SwarmVariable">SwarmVariable</a></code></h4>
<ul class="two-column">
<li><code><a title="underworld3.swarm.SwarmVariable.data" href="#underworld3.swarm.SwarmVariable.data">data</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.rbf_interpolate" href="#underworld3.swarm.SwarmVariable.rbf_interpolate">rbf_interpolate</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.read_timestep" href="#underworld3.swarm.SwarmVariable.read_timestep">read_timestep</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.save" href="#underworld3.swarm.SwarmVariable.save">save</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.sym" href="#underworld3.swarm.SwarmVariable.sym">sym</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.sym_1d" href="#underworld3.swarm.SwarmVariable.sym_1d">sym_1d</a></code></li>
<li><code><a title="underworld3.swarm.SwarmVariable.write_proxy" href="#underworld3.swarm.SwarmVariable.write_proxy">write_proxy</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
